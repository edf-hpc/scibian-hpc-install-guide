=== Ceph deployment

Deployment is based on a tool called `ceph-deploy`. This tool performs the
steps on a node to setup a ceph component. It is only used for the initial setup
of the Ceph cluster. Once the cluster is running, the configuration is reported
in the Puppet configuration in case it is re-deployed.

The reference configuration uses one disk (or hardware RAID LUN) to hold the
system (`/dev/sda`) and another to hold the Ceph OSD data and journal
(`/dev/sdb`). Three or five nodes must be chosed to setup the *MON* and *MDS*
services, the remaining nodes are used only as *OSD* and *RadosGW* nodes.

The `ceph-deploy` utility generates authentication keys for Ceph. Once the
cluster is running, theses keys are manually collected and encrypted with
`eyaml` to be included in the *hiera* configuration.

In the following example MONs/MDS are installed on nodes `fbservice[2-4]` while
the node `fbservice1` only has OSD and RGW.

==== Packages installation

Install the `ceph-deploy` utility and the S3 CLI client `s3cmd`:

[source,bash]
----
apt-get install ceph-deploy s3cmd
----

The deployment of Ceph cluster generates a bunch of files (keyrings,
configuration file, etc). Create a temporary directory to store these files:

[source,bash]
----
mkdir ~root/ceph-deploy && cd ~root/ceph-deploy
----

Install the Ceph software stack on all nodes of the Ceph cluster:

[source,bash]
----
ceph-deploy install --no-adjust-repos $(nodeset -e @service)
----

==== Cluster bootstrap

Initialize the cluster with the first MON server of the Ceph cluster in
parameter:

[source,bash]
----
ceph-deploy new \
    --public-network <administration network address> \
    --cluster-network <administration network address> \
    fbservice2
ceph-deploy mon create-initial
----

Install admin credentials

[source,bash]
----
ceph-deploy admin $(nodeset -e @service)
----

Create the MON servers:

[source,bash]
----
ceph-deploy mon add fbservice3
ceph-deploy mon add fbservice4
----

Create the OSD servers:

[source,bash]
----
ceph-deploy disk zap $(nodeset -O %s:sdb -e @service)
ceph-deploy osd prepare $(nodeset -O %s:sdb -e @service)
----

Create the MDS servers:

[source,bash]
----
ceph-deploy mds create $(nodeset -e fbservice[2-4])
----

Check the Ceph cluster status:

[source,bash]
----
ceph status
----

The command must report `HEALTH_OK`.

==== RadosGW

Enable RadosGW with the following command:

[source,bash]
----
ceph-deploy rgw create $(nodeset -e @service)
----

==== Libvirt RBD pool

The virtual machines will use a specific libvirt storage pool to store the disk
images. This libvirt storage pool uses ceph RBD, so a specific ceph pool is
necessary. This is not handled by `ceph-deploy`:

[source,bash]
----
ceph osd pool create libvirt-pool 64 64
----

If the cluster has five OSDs or more, the numbers of PG and PGP can be set to
128 instead of 64.

The client credentials must be manually generated:

[source,bash]
----
ceph auth get-or-create client.libvirt \
    mon 'allow r' \
    osd 'allow class-read object_prefix rbd_children, allow rwx pool=libvirt-pool'
----

==== CephFS initialization

In high-availability mode, Slurm controller requires a shared POSIX filesystem
between the primary and the backup controllers. In the Scibian HPC cluster
reference architecture, CephFS is used for this filesystem. Create this CephFS
filesystem with the following commands:

----
# ceph osd pool create cephfs_data 64 64
pool 'cephfs_data' created
# ceph osd pool create cephfs_metadata 64 64
pool 'cephfs_metadata' created
# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 15 and data pool 14
----

If the cluster has five OSDs or more, the numbers of PGs can be set to 128 for
data and metadata pool.

==== RadosGW S3

A user must be created to access the RadosGW S3 API:

[source,bash]
----
radosgw-admin user create --uid=hpc-config --display-name="HPC Config push"
----

This commands gives an `access_key` and a `secret_key` that can be used by
`hpc-config-push(1)` or `s3cmd(1)`.

Create a temporary configuration file for s3cmd with these keys:

----
# cat <<EOF >~/.s3cfg
[default]
access_key=<ACCESS_KEY>
secret_key=<SECRET_KEY>
host_bucket=%(bucket)s.service.virtual:7480
host_base=rgw.service.virtual:7480
use_https=False
EOF
----

With the `access_key` and the `secret_key` provided by `radosgw-admin user
create` command.

To work properly with Amazon S3 tools and consul DNS, RadosGW must be
configured to accept requests on `rgw.service.virtual` and on
`<bucket_name>.service.virtual`. To configure this, it is necessary to
re-define the default realm, region and zonegroup.

The region is configured by writing a JSON region file (`rgw-region.json`):

[source,json]
----
include::../../examples/rgw-region.json[]
----

Inject this region file into RadosGW configuration:

[source,bash]
----
radosgw-admin realm create --rgw-realm=default --default
radosgw-admin region set --infile rgw-region.json
radosgw-admin region default --rgw-zonegroup=default
radosgw-admin zonegroup add --rgw-zonegroup=default --rgw-zone=default
----

Define default zone and zonegroup:

[source,bash]
----
radosgw-admin zone default --rgw-zone=default
radosgw-admin zonegroup default --rgw-zonegroup=default
----

Update the period:

[source,bash]
----
radosgw-admin period get
radosgw-admin period update --commit
----

After this step the RadosGW daemons must be restarted on every nodes:

[source,bash]
----
clush -g service 'systemctl restart ceph-radosgw@rgw.${HOSTNAME}.service'
----

Finally, create the bucket with `s3cmd`:

----
# s3cmd mb --acl-public s3://s3-system
Bucket 's3://s3-system/' created
----

==== Transfer to Hiera

When the Ceph cluster is fully initialized, its configuration must be reported
to the Hiera repository. First, general topology information must be reported
into the cluster specific layer of the hiera repository
`$ADMIN/hpc-privatedata/hieradata/$CLUSTER/cluster.yaml`, for example:

[source,yaml]
----
include::../../examples/hiera-ceph-cluster.yaml[]
----

In this example, the `<fsid>` must be replaced with the value obtained with the
following command:

[source,bash]
----
ceph fsid
----

Then, all keyrings must be reported in the area YAML file
`$ADMIN/hpc-privatedata/hieradata/$CLUSTER/areas/$AREA.yaml` whose generic
service nodes are members (ex: _default_ or _infra_), using `eyaml` :

[source,yaml]
----
include::../../examples/hiera-ceph-area.yaml[]
----

The bootstrap keys have been generated in the temporary Ceph deployment
directory:

[source,bash]
----
cd ~root/ceph-deploy
cat ceph.client.admin.keyring
cat ceph.mon.keyring
cat ceph.bootstrap-mds.keyring
cat ceph.bootstrap-osd.keyring
cat ceph.bootstrap-rgw.keyring
----

The OSD keys can be gathered with:

[source,bash]
----
clush -bg service 'cat /var/lib/ceph/osd/ceph-?/keyring'
----

The MDS keys can be gathered with:

[source,bash]
----
clush -bg service 'cat /var/lib/ceph/mds/ceph-${HOSTNAME}/keyring'
----

The RGW keys can be gathered with:

[source,bash]
----
clush -bg service 'cat /var/lib/ceph/radosgw/ceph-rgw.${HOSTNAME}/keyring'
----

Then, add the `ceph::server` profile into the service role:

[source,diff]
----
--- a/hpc-privatedata/hieradata/foobar/roles/service.yaml
+++ b/hpc-privatedata/hieradata/foobar/roles/service.yaml
@@ -28,5 +28,6 @@
   - profiles::bootsystem::server
   - profiles::dhcp::server
   - profiles::environment::limits
+  - profiles::ceph::server
 
 profiles::network::gw_connect: 'wan'
----

Then push the new configuration:

[source,bash]
----
hpc-config-push
----

Theoritically, at this stage, the Ceph cluster can be fully configured with
Puppet. It is really recommended to check this by re-installing one of the
generic service nodes (excepting the temporary installation node) before going
further. Please mind that in case of generic service node reinstallation after
the initial configuration, bootstrap steps may be necessary:

- *MDS* and *RadosGW*, those services have no state outside of Rados, so no
  additional bootstrap is necessary
- *Mon* Always necessary to bootstrap
- *OSD* Must be bootstraped if the OSD volume (`/dev/sdb`) is lost.

Please refer to the bootstrap procedure section for all details.

Once the re-installation of a generic service node with Ceph is validated, the
`ceph-deploy` temporary directory can be removed from the temporary installation
node:

[source,bash]
----
rm -r ~root/ceph-deploy
----

==== Network restrictions

By default with Puppet-HPC, Ceph daemons socket are binded to the administration
network interface of the generic service nodes. This setup is done on purpose
for security reasons and avoid access to the Ceph cluster from outside of the
administration network (typically from the wan network, outside of the cluster).

However, this can be easily changed by overriding this parameter in the hiera
repository:

[source,yaml]
----
profiles::ceph::listen_network: 'wan' # Make ceph listen the wan network for
                                      # connections, default is 'administration'
----

It is also possible to totally disable the network restriction settings on Ceph
daemons with:

[source,yaml]
----
ceph::restrict_network: false
----
