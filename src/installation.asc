= Installation procedures

The current chapter describes how to install the Puppet-HPC software stack
used to configure the administration and generic nodes of the HPC system.
This chapter also explains how to use Ceph for sharing the configuration
files across all the nodes, how to handle the virtual machines providing
all the services needed to operate the HPC system and any other step needed.

== Set up Configuration Management

The first step in installing a Scibian HPC cluster is to have an operational
Configuration Management functionality. In Scibian HPC clusters, this function
is based on the Puppet-HPC software stack.

=== Internal repository

////
TODO: document the steps to initialize the internal hieradata repository
////

In addition to
https://github.com/edf-hpc/puppet-hpc[the GitHub repository of Puppet-HPC],
an internal Git repository containing all specific data is necessary. If this
internal repository does not exist yet, it should be created on an internal
forge or a Git server. It must be accessible from the admin node of the cluster
being installed.

Its structure and content are detailed in
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC reference documentation],
in Software Architecture, Internal Repository.

For example, if you are installing a cluster `cluster1` and creating its
internal repository, its layout should look like this.

----
[git_server]$tree internal_rep
internal_rep/
|-- files
|   `-- cluster1
|-- hieradata
|   `-- cluster1
`-- puppet-config
    `-- cluster1
----

=== Configure Puppet and Hiera

////
TODO: document how-to write puppet.conf, hiera.yaml and which functionalities
are necessary (eyaml) with reference to [Puppet-HPC reference documentation]
////

The __puppet-config/cluster1__ subdirectory of the internal Git repository
should contain all the files necessary to set up Puppet and Hiera on each node
of `cluster1`.

==== puppet.conf

The Puppet configuration file, __puppet.conf__ common to all the nodes should be
stored in __puppet-config/cluster1__.
For example, if Puppet < 4 is used in your environment, it is required to add
the setting `stringify_facts=false` to __puppet.conf__ so that you can use
Puppet-HPC, as specified in
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC reference documentation],
in Development Guidelines, Language Settings.

Here is below the typical content of the __puppet.conf__ file to add to
__puppet-config/cluster1__ when the version of Puppet is < 4.

----
[main]
logdir=/var/log/puppet
vardir=/var/lib/puppet
ssldir=/var/lib/puppet/ssl
rundir=/var/run/puppet
environmentpath = $confdir/environments
prerun_command=/etc/puppet/etckeeper-commit-pre
postrun_command=/etc/puppet/etckeeper-commit-post
stringify_facts=false
hiera_config=$confdir/hiera.yaml

[master]
# These are needed when the puppetmaster is run by passenger
## and can safely be removed if webrick is used.
# ssl_client_header = SSL_CLIENT_S_DN
# ssl_client_verify_header = SSL_CLIENT_VERIFY
----

==== hiera.yaml

The Hiera configuration file, __hiera.yaml__ should also be common to all the
cluster nodes. It is usually stored in __puppet-config/cluster1__.

It is used to indicate which backend should be used for Hiera and the hierarchy
to take into account. If Hiera-Eyaml is used to encrypt some data, it should
also be specified in this file.

Puppet-HPC uses the `yaml` backend of Hiera and encrypts passwords with
Hiera-Eyaml. A typical __hiera.yaml__ configuration file for Puppet-HPC would
look like the following.

----
:backends:
  - eyaml
:eyaml:
  :datadir:           /etc/puppet/environments/%{environment}/hieradata
  :pkcs7_private_key: /etc/puppet/secure/keys/private_key.pkcs7.pem
  :pkcs7_public_key:  /etc/puppet/secure/keys/public_key.pkcs7.pem
  :extension:         'yaml'
:hierarchy:
  - private/%{cluster_name}/roles/%{puppet_role}
  - private/%{cluster_name}/cluster
  - private/%{cluster_name}/network
  - private/edf_hpc
  - generic/common
  - generic/%{osfamily}/common
----

More information about how Hiera-Eyaml is used to deal with sensitive data
in Puppet-HPC can be found in
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC reference documentation],
in Software Architecture, Sensitive Data Encryption.

==== hpc-config-facts.yaml

This file can be used to define additional facts common to all the nodes of the
cluster, for example the name of the cluster. This file should also be stored
in __puppet-config/cluster1__ of the internal Git repository.

Here is an example of __hpc-config-facts.yaml__

----
---
cluster_name: 'cluster1'
other_cluster_fact: 'toto'
----

It is copied by __hpc-config-apply__ locally on each node under
__/var/lib/puppet/facts.d__ so that its content is published as facts.

=== Cluster definition

////
TODO: document how-to write cluster definition in hiera, with reference to
[Puppet-HPC reference documentation]
////

Still in the case of a cluster named `cluster1`, the __hieradata/cluster1__
subdirectory of the internal Git repository should contain the
full description of the cluster being installed, also referred to as cluster
definition.

This section guides you through the steps to define your cluster in Hiera.
More detailed information can be found in
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC reference documentation],
in Software Architecture, Cluster Definition.

==== Global description of the cluster

First, the main shared parameters defining your cluster must be specified. This
is usually done in a file named __cluster.yaml__.

----
## Global description of the cluster
cluster_name:      'cluster1'                       # Name of the cluster
cluster_prefix:    'cl'                             # Prefix used in node names
private_files_dir: "/admin/production/latest/files" # Where configuration files are stored
domain:            'cluster1.hpc.domain.org'        # Domain name used accross all the
                                                    # machines
user_groups:                                        # Array of user groups allowed to access
  - "cl-%{hiera('cluster_name')}-users-dep1"        # to the cluster
  - "cl-%{hiera('cluster_name')}-users-dep2"
  - "cl-%{hiera('cluster_name')}-users-dep2"
cluster_decrypt_password:
                    'my_encrypted_password'         # Password encrypted with Hiera-Eyaml
----

==== Cluster networks

Once these parameters have been defined, the different networks used in the
cluster must be described. The 2 files containing the network details are
__cluster.yaml__ and __network.yaml__.

===== In cluster.yaml

====== Topology

The network topology is usually specified in the file __cluster.yaml__.
As seen earlier in this document, a cluster will typically have:

* some nodes connected to the WAN network,
* a local low-latency network efficient for calculations,
* a local administration network including a VLAN dedicated management devices such as Board Management Cards (BMC).

====== Bonding

If bonding is used between some network interfaces on some nodes of the
cluster, it must be defined in __cluster.yaml__. The `network::bonding_options`
should be used to specify the options for each bond.

====== Bridges

In the same way, it is also possible to set up bridge interfaces.
The specification should be done by defining the `network::bridge_options`.

====== Example

A full example of network definition in __cluster.yaml__ is detailed in
the section Network Definitions of
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC Reference].

===== In network.yaml

The file __network.yaml__ includes a hash called `master_network` that lists a
description of network settings for all the nodes in the cluster. A detailed
example is available in the section Node Definitions of
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC Reference].

=== Generate keys

////
TODO: document the steps to generate the keys, certificates, and so on.
////

==== The Cluster Decrypt Password and keys for data encryption

As explained in the Puppet-HPC reference, sensitive data stored in the internal
Git repository should be encrypted. Puppet-HPC uses the 2 following methods to
do so:

* Full sensitive files are encrypted with the `enc` plugin of the tool `Clara`
* Passwords are directly encrypted in Hiera input files using the Hiera-Eyaml
functionality

Consequently you will need 2 types of encryption keys:

* a PKCS7 key pair to set up Hiera-Eyaml,
* an AES key that is used by Clara to encrypt files, and by Puppet-HPC to
  decrypt them. This unique password is referred to as the `Cluster Decrypt
  Password` in Puppet-HPC. It is used to encrypt any configuration file that
  needs to be.

To generate the PKCS7 key pair, it is necessary to work on a node where
Hiera-Eyaml has been installed manually. The full procedure to initially
generate these keys is explained in
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC Reference] in Sensitive Data Encryption, Bootstrap Procedure.

The PKCS7 key pair is expected to be tared, encoded and then stored in the
subdirectory __files/cluster1/eyaml__ of the internal repository.

Furthermore, please refer to
http://edf-hpc.github.io/clara/[Clara documentation] if you need more details on
how it works.

==== Self-signed certificate

A production system should use a certificate validated by the operating system,
either through a public CA or one internal to the cluster organization. It is
possible to generate a self-signed certificate when that is not possible or for
testing purposes.

----
# openssl req -x509 -newkey rsa:2048 -keyout ssl-cert-generic.key \
  -out ssl-cert-generic.pem -days 3650 -nodes
Generating a 2048 bit RSA private key
...............................+++
....+++
writing new private key to 'ssl-cert-generic.key'
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:FR
State or Province Name (full name) [Some-State]:France
Locality Name (eg, city) []:Paris
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Example
Organizational Unit Name (eg, section) []:HPC
Common Name (e.g. server FQDN or YOUR name) []:clservice1.gen.hpc.example.com
Email Address []:Example HPC <hpc@example.com>
----

The Puppet HPC configuration will use this certificate in an encrypted form.
To encrypt it, it is possible to use `clara enc encode` directly. Otherwise
the following command can perform the encryption:

----
# openssl aes-256-cbc -in ssl-cert-generic.key -out ssl-cert-generic.key.enc \
  -k <cluster decrypt password>
----

==== SSH key generation

To be able to reuse the same host keys between node re-installations, they are
stored encrypted in the subdirectory __files/cluster1/hostkeys__ of the internal
Git repository. It is possible to generate keys by cluster, role or host. A
cluster should have at least a default key.

----
# cd hpc-privatedata/files/cluster1/hostkeys
# ssh-keygen  -t dsa -N '' -f ssh_host_dsa_key
# ssh-keygen  -t ecdsa -N '' -f ssh_host_ecdsa_key
# ssh-keygen  -t ed25519 -N '' -f ssh_host_ed25519_key
# ssh-keygen  -t rsa -N '' -f ssh_host_rsa_key
# for i in * ; mv $i $i.default ; done
----

Last step is to encrypt the keys with the Cluster Decrypt Password. It is
possible to do so with `clara enc encode` directly.
Otherwise the following command can perform the encryption:

----
# openssl aes-256-cbc -in ssh_host_ed25519_key.default \
  -out ssh_host_ed25519_key.default.enc -k <cluster decrypt passw
----

== Install admin node

////
TODO: document the steps to boot the admin node with rikenter and
install/configure minimal installation system in live.
////
__To be added upon IQ installation__

== Install generic service nodes

////
TODO: document how-to install and configure generic  nodes.
////
__To be added upon IQ installation__

== Configure Ceph

=== Architecture

image::src/img/ceph_architecture.svg[image]

A ceph cluster is created on the service nodes. A ceph cluster is made of four
kinds of daemons. All service nodes will have:

- *OSD*, Object Storage Daemons actually holding the content of the ceph
  cluster
- *RGW*, Rados GateWay (sometimes shortened radosgw) exposing an HTTP API like
  S3 to store and retrieve data in Ceph

Two other kind of service are only available on three of the service nodes:

- *MON*, Monitoring nodes, this is the orchestrator of the ceph cluster. A
  quorum of two active mon nodes must be maintained for the cluster to be
  available
- *MDS*, MetaData Server, only used by CephFS (the POSIX implementation above
  ceph). At least one must always be active.

With this configuration, any server can be unavailable. As long as at least two
servers holding critical services are available, the cluster might survive
losing another non-critical server.

=== Deployment

Deployment is based on a tool called `ceph-deploy`. This tool performs the
steps on a node to setup a ceph component. `ceph-deploy` is used to setup a
node, once the cluster is running, the configuration is reported in the Puppet
configuration in case it is re-deployed.

The reference configuration uses one disk (or hardware RAID LUN) to hold the
system (`/dev/sda`) and another to hold the Ceph OSD data and journal
(`/dev/sdb`).

Three or five nodes must be chosed to setup the *MON* and *MDS* services, the
remaining nodes are used only as *OSD* and *RadosGW* nodes.

To work, `ceph-deploy` must run on a node with a passwordless SSH connection to
the service nodes, directly to root or to a user with passwordless sudo.

The `ceph-deploy` tool will generate authentication keys for ceph. Once the
cluster is running, theses keys should be manually collected and encrypted with
`eyaml` to be included in the *hiera* configuration.

In the following example MONs/MDS are installed on nodes `clservice[2-4]` and a
node `clservice1` only has OSD and RGW.

==== Ceph-deploy init

----
# mkdir ceph-deploy
# cd ceph-deploy
# ceph-deploy new clservice2
----

==== Manually install packages

----
# ceph-deploy install \
  --repo-url http://repo.hpc.example.com/scibian-hpc/scibian8/scibian8 \
  --no-adjust-repos clservice1 clservice2 clservice3 clservice4
----

==== Install admin credentials

----
# ceph-deploy admin clservice1 clservice2 clservice3 clservice4
----

==== MON

----
for i in {2..4} ; do ceph-deploy mon add clservice${i} ; done
----

==== OSD

----
# for i in {1..4} ; do ceph-deploy disk zap clservice${i}:sdb ; done
# for i in {1..4} ; do ceph-deploy osd prepare clservice${i}:sdb ; done
----

==== MDS

----
# ceph-deploy mds create clservice2 clservice3 clservice4
----

==== RadosGW

----
# for i in {1..4} ; do ceph-deploy rgw create clservice${i} ; done
----

==== Libvirt RBD pool

The virtual machines will use a specific libvirt storage pool to store the disk
images. This libvirt storage pool uses ceph RBD, so a specific ceph pool is
necessary. This is not handled by `ceph-deploy`:

----
# ceph osd pool create libvirt-pool 64 64
----

If the cluster has five OSDs or more, the numbers of PGs can be set to 128.

The client credentials must be manually generated:

----
# ceph auth get-or-create client.libvirt \
    mon 'allow r' \
    osd 'allow class-read object_prefix rbd_children, allow rwx pool=libvirt-pool'
----

==== CephFS initialization

----
# ceph osd pool create cephfs_data 64
pool 'cephfs_data' created
# ceph osd pool create cephfs_metadata 64
pool 'cephfs_metadata' created
# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 15 and data pool 14
----

If the cluster has five OSDs or more, the numbers of PGs can be set to 128 for
data and metadata pool.

==== RadosGW S3

A user must be created to access the RadosGW S3 API:

----
# radosgw-admin user create --uid=hpc-config --display-name="HPC Config push"
----

This commands gives an `access_key` and a `secret_key` that can be used by
`hpc-config-push(1)` or `s3cmd(1)`.

To work properly with Amazon S3 tools and consul DNS, RadosGW must be
configured to accept requests on `rgw.service.virtual` and on
`<bucket_name>.service.virtual`. To configure this, it is necessary to
re-define the default realm, region and zonegroup.

The region is configured by writing a JSON region file (`rgw-region.json`):

[source]
----
include::examples/rgw-region.json[]
----

----
# radosgw-admin realm create --rgw-realm=default --default
# radosgw-admin region set --infile rgw-region.json
# radosgw-admin region --rgw-zonegroup=default default
# radosgw-admin zonegroup default --rgw-zonegroup=default
# radosgw-admin period get
# radosgw-admin period update --commit
----

After this step the RadosGW daemons must be restarted on every nodes.

----
clush -w clservice[1-4] 'systemctl restart ceph-radosgw@rgw.${HOSTNAME}.service''
----

==== Hiera

When the cluster is initialized, the authentication keys must be reported in the
Hiera configuration.

[source]
----
include::examples/hiera-ceph.yaml[]
----

=== Ceph server re-installation

In case of generic service node reinstallation after the initial configuration,
bootstrap steps may be necessary:

- *MDS* and *RadosGW*, those services have no state outside of Rados, so no
  additional bootstrap is necessary
- *Mon* Always necessary to bootstrap
- *OSD* Must be bootstraped if the OSD volume (`/dev/sdb`) is lost.

== Re-install administration node

Once the Service nodes are fully configured (Ceph, DNS, Consul, DHCP, TFTP,
HTTP for boot...), the cluster is able to reinstall any physical or virtual
machine. This include service nodes and the admin node.

The early stages of a cluster installation may require some temporary settings
and services to work around some issues. The bootstrap sequence itself creates
such services. To make sure that all the configuration is safely stored, the
next step is to reinstall the administration node. Keeping a copy (`rsync(1)`)
of the administration node file system is recommended, at least until the node
is working again as expected.

The procedure to reinstall the administration node is documented in the
<<production-admin-reinstall, Production section>>.

== Install virtual machines

////
TODO: document how-to install and configure virtual machines with references to
[bootstraping guides] for service.
////

The generic services machines belongs to a ceph cluster and can host different
virtual machines to handle all the services needed to operate the HPC cluster.

Each virtual machine is defined on a generic service machine, and his disk is
stored in the ceph shared storage.

It is possible to create, delete and move virtual machines between the generic
servers with the `Clara` tool.

[[img-arch_service_vm]]
.How the service machines, ceph and vm interact
image::src/img/arch_service_vm.svg[image]


=== Clara configuration

`Clara` uses the `libvirt` tool to manage virtual machines. __clara virt__
configuration is based on a file __virt.ini__ and a directory __templates__,
both placed in the `Clara` configuration directory.

Here is an example of the __virt.ini__ file:

----
[nodegroup:default]
default=true
nodes=genservice1,genservice2,genservice3,genservice4

[pool:default]
vol_pattern={vm_name}_{vol_role}.qcow2
default=false

[template:default]
default=true
xml=default_generic.xml
vol_roles=system
vol_role_system_capacity=60000000000
networks=administration

[pool:rbd-pool]
default=true
vol_pattern={vm_name}_{vol_role}

[template:proxy]
vm_names=genproxy[1,2]
xml=default_generic.xml
vol_roles=system
vol_role_system_capacity=60000000000
networks=administration,wan
core_count=16
----

It defines the hosts used to run the virtual machines, a default template for
all the vms, and specific templates for the vms when necessary.

The xml templates used in __virt.ini__ must be placed in the __templates__
directory. At least one default vm template and one default volume template
must be defined.
See the `Clara` documentation detailed informations.

=== Instanciate and install a VM

From the `Scibian HPC` point of view, there is nothing specific between a
virtual machine and a physical machine. Both must be defined in the __cluster.yaml__
file and have a role (and profiles) assigned, as specified in
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC reference documentation],
in Software Architecture, Cluster Definition.
The installation process is also identical.

`Clara` is used to create a virtual machine:

----
# clara virt define genproxy1 --host=genservice1
----

Then start the virtual machine to install it:

----
# clara virt start genproxy1
----

If the disk volume is empty, the machine will boot on lan with pxe. If the disk
volume is not empty, it is necessary to add the `--wipe` parameter.
Once installed, the virtual machine reboot and is ready for production.

==== Specific roles

Some of the virtual machines provides services who need a specific bootstrap
procedure, as described in <<bootstrap-procedures,Bootstrap procedures>>.
These machines are:
* the *proxy* machines, with the ldap bootstrap procedure
* the *batch* machines with the MariaDB/Galera bootstrap procedure and the
Slurmdbd bootstrap procedure.


== Build diskless image

Diskless nodes use squashfs images downloaded at boot. A simple way to
generate these images is to use https://github.com/edf-hpc/clara[`clara`].
Once configured, `clara image create` can be used to build Debian image(s).

Clara can install a selected list of packages into the image, manage
the repositories used and add specific files and directories.

Example:

----
# clara image create scibian8
----

Clara can also generate an `initrd` (initial ramdisk) to use with the image.
It can download the image either by HTTP or BitTorrent protocol.

Example:
----
# clara image initrd scibian8
----

Initrd must be regenerated each time the kernel image is modified.

See the http://edf-hpc.github.io/clara/[clara documentation]  for a full
list of options and possibilities.

Every time the image and the initrd are regenerated, they must be made
available to the deployment system.

== Boot nodes

To remotely start the nodes, you can use https://github.com/edf-hpc/clara[`clara`].
The ipmi plugin from clara is used for powering on/off selected nodes,
managing the boot node mode (if you want to install OS on disk or load it in RAM)
and accessing to the remote console.

See the http://edf-hpc.github.io/clara/[clara documentation] for a full
list of options and possibilities.

The commands from the following example, show how to reboot a node with
the boot menu, choose the operating system and boot mode between "diskless"
or "on disk".

Example:

----
# clara ipmi pxe <node>
# clara ipmi reboot <node>
# clara ipmi connect <node>
----

After the last command, when the login prompt is shown, you can connect on node
as root and monitor the progress of the puppet configuration by running
the following command:

----
# journalctl -f -u hpc-config-apply
----

When the network is up in the node you can also disconnect from the console
(press on "!" and "." keys as explained on the clara documentation) and
connect to the node via ssh.

==== Optional: internal APT repository

The cluster must use a private cluster keyring. This keyring is used to
sign packages generated locally and in the local repositories.

You should generate it and store it in the internal repository in the
subdirectory __files/cluster1/repo__. You will be asked for a
passphrase, this passphrase must be provided interactively when you call
`clara repo add|del`. The following command can be pretty long to
execute if you do not use a hardware Random Number Generator (RNG).

----
# LANG=C gpg --no-default-keyring --keyring files/repo/cluster_keyring.gpg \
    --secret-keyring files/repo/cluster_keyring.secret.gpg --gen-key
gpg (GnuPG) 1.4.18; Copyright (C) 2014 Free Software Foundation, Inc.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

gpg: keyring `files/repo/cluster_keyring.secret.gpg' created
gpg: keyring `files/repo/cluster_keyring.gpg' created
Please select what kind of key you want:
   (1) RSA and RSA (default)
   (2) DSA and Elgamal
   (3) DSA (sign only)
   (4) RSA (sign only)
Your selection? 1
RSA keys may be between 1024 and 4096 bits long.
What keysize do you want? (2048) 4096
Requested keysize is 4096 bits
Please specify how long the key should be valid.
         0 = key does not expire
      <n>  = key expires in n days
      <n>w = key expires in n weeks
      <n>m = key expires in n months
      <n>y = key expires in n years
Key is valid for? (0)
Key does not expire at all
Is this correct? (y/N) y

You need a user ID to identify your key; the software constructs the user ID
from the Real Name, Comment and Email Address in this form:
    "Heinrich Heine (Der Dichter) <heinrichh@duesseldorf.de>"

Real name: HPC Team Example cluster
Email address: hpc@example.com
Comment:
You selected this USER-ID:
    "HPC Team Example cluster <hpc@example.com>"

Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O
You need a Passphrase to protect your secret key.

passphrase not correctly repeated; try again.
We need to generate a lot of random bytes. It is a good idea to perform
some other action (type on the keyboard, move the mouse, utilize the
disks) during the prime generation; this gives the random number
generator a better chance to gain enough entropy.
..+++++
...........+++++
We need to generate a lot of random bytes. It is a good idea to perform
some other action (type on the keyboard, move the mouse, utilize the
disks) during the prime generation; this gives the random number
generator a better chance to gain enough entropy.
+++++
.+++++
gpg: key 241FB865 marked as ultimately trusted
public and secret key created and signed.

gpg: checking the trustdb
gpg: public key of ultimately trusted key 1F2607DD not found
gpg: public key of ultimately trusted key 94DEFA86 not found
gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model
gpg: depth: 0  valid:   3  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 3u
pub   4096R/241FB865 2016-05-19
      Key fingerprint = D192 11C0 2EB6 BE80 A3BC  7928 1CB4 3266 241F B865
uid                  HPC Team Example cluster <hpc@example.com>
sub   4096R/C7027D3A 2016-05-19
----

Clara uses this key in its encrypted form. If the `clara enc` plugin is set,
 it is possible to use `clara enc encode` directly. Otherwise the
following command can perform the encryption:

----
# openssl aes-256-cbc -in cluster_keyring.secret.gpg \
  -out cluster_keyring.secret.gpg.enc -k <cluster decrypt password>
----
