[[inst-slurm]]
== Slurm

Slurm workload manager is distributed among the HPC cluster nodes with multiple
daemons and clients software. On Scibian HPC clusters, the server part of
Slurm, _ie._ the controller and the accounting services, run in
high-availability mode on the _batch_ nodes. These components are managed by
the `jobsched::server`.  The _batch_ nodes also need the `db::server`, and the
`ceph::client` or `nfs::mount` that respectively setup the MariaDB galera
RDBMSfootnote:[Relational Database Management System], and CephFS or NFS
filesystem client.

Slurm communications between nodes are secured using Munge which is based on a
secret shared key. Generate this munge key with the following command:

----
# mkdir -p $ADMIN/hpc-privatedata/files/$CLUSTER/munge
# dd if=/dev/urandom bs=1 count=1024 > $ADMIN/hpc-privatedata/files/$CLUSTER/munge/munge.key
----

Encrypt the key using Clara:

----
# clara enc encode $ADMIN/hpc-privatedata/files/$CLUSTER/munge/munge.key
----

Remove the unencrypted key:

----
# rm $ADMIN/hpc-privatedata/files/$CLUSTER/munge/munge.key
----

Setup the nodes and partitions managed by Slurm in the
`slurm::partitions_options` hash in the cluster specific layer of the Hiera
repository. For example:

[source,yaml]
----
slurm::partitions_options:
  - 'NodeName=fbcn[01-04] Sockets=2 CoresPerSocket=14 RealMemory=64000 State=UNKNOWN'
  - 'NodeName=fbgn01 Sockets=2 CoresPerSocket=4 RealMemory=64000 Gres=gpu:k80:2 State=UNKNOWN'
  - 'PartitionName=cn Nodes=fbcn[01-04] Default=YES MaxTime=INFINITE State=UP'
  - 'PartitionName=gn Nodes=fbgn01 MaxTime=INFINITE State=UP'
  - 'PartitionName=all Nodes=fbcn[01-04],fbgn01 MaxTime=INFINITE State=UP'
----

Please refer to https://slurm.schedmd.com/[Slurm documentation] for more details
about these settings.

In the same, setup the LDAP/SlurmDBD users synchronization utility, for example:

[source,yaml]
----
profiles::jobsched::server::sync_options:
  main:
    cluster: "%{hiera('cluster_name')}"
    org:     "%{hiera('org')}"
    policy:  'global_account'
  global_account:
    name:    'users'
    desc:    'Main users account'
----

Please refer to the
https://github.com/edf-hpc/slurm-llnl-misc-plugins/blob/master/sync-accounts/sync-accounts.conf[example
configuration file] for more details.


Still in the cluster specific layer of the Hiera repository, setup  the shared
storage directory.

If you are using CephFS, configure the client mount with the following excerpt:

[source,yaml]
----
profiles::jobsched::server::ceph::keys:
  client:
    key: "%{hiera('ceph_client_admin_key')}"

profiles::jobsched::server::ceph::mounts:
  slurmctld:
    servers: # list of Ceph MON servers
      - fbservice2
      - fbservice3
      - fbservice4
    device:     '/slurmctld'
    mountpoint: "%{hiera('slurm_state_save_loc')}"
    user:       'admin'
    key:        'client'
    mode:       'kernel'
----

If you are using an NFS HA Server:
[source,yaml]
----
profiles::jobsched::server::ceph::enabled: false

profiles::jobsched::slurm_config_options:
  [...]
  StateSaveLocation:        '/admin/restricted/backup/slurm_state_save' 
----

For NFS HA, at the role level, configure the NFS mount:
----
profiles:
  [...]
  - profiles::nfs::mounts

profiles::nfs::to_mount:
  home:
    server:     'fbnas'
    exportdir:  '/srv/admin'
    mountpoint: '/admin'
    options:    'bg,rw,hard,vers=4'
----

Eventually, it is possible to tune Slurm, GRES, SlurmDBD, job submit LUA script
with the following parameters:

[source,yaml]
----
profiles::jobsched::slurm_config_options:
  PrivateData:              'jobs,reservations,usage'
  AccountingStorageEnforce: 'associations,limits,qos'
  GresTypes:                'gpu'
  SlurmCtldDebug:           'verbose'
  PriorityFlags:            'FAIR_TREE'

slurm::gres_options:
  - 'NodeName=fbgn01 Name=gpu Type=k80 File=/dev/nvidia0'

profiles::jobsched::server::slurmdbd_config_options:
  PrivateData: 'accounts,jobs,reservations,usage,users'

slurm::ctld::submit_lua_options:
  CORES_PER_NODE:  '28'
----

Once the configuration is set in the Hiera repository, push and apply the
configuration on the _admin_ and _batch_ nodes:

----
# hpc-config-push && clush -bg admin,batch hpc-config-apply -v
----

By default, the MariaDB server is setup with parameters to harden its security.
Notably, the following settings are deployed by default:

* `max_user_connections` to 100 (default is 0 ie. unlimited), in order to
  prevent one user from grabbing all 151 available `max_connections` (default
  MariaDB value).
* `secure_file_priv` is set to an empty value in order to disable potentially
  dangerous command `LOAD DATA INFILE`.
* the client histfile `~/.mysql_history` is disabled by default.

Obviously, these settings can be altered in the hiera repository. Here is an
example yaml excerpt to change these default values:

----
mariadb::disable_histfile: false
mariadb::galera_conf_options:
  mysqld:
    max_user_connections: '0' # unlimited
    secure_file_priv:     '/'
----

Some software components need to be manually bootstrapped on the _batch_ nodes
before being started:

* <<bootstrap-mariadb,MariaDB>> database
* <<bootstrap-slurmdbd,SlurmDBD>> service

The shared storage can be on CephFS or on NFS HA, the suitable bootstrap procedure
must be performed:

* <<bootstrap-ceph-fs,CephFS>> filesystem
* <<bootstrap-nfs-ha,NFS HA>> filesystem

Please refer to the <<bootstrap,Bootstrap procedure chapter>> of this document
for all details.

Finally, re-apply the configuration on the _batch_ nodes to launch all Slurm
server-side software services with:

----
# clush -bg batch hpc-config-apply -v
----

Check Slurm is available by running the `sinfo` command on the admin node. If
the command report the nodes and partitions state without error, Slurm is
properly running.
