=== Migrate configuration

At this stage, the configuration was published on the HTTP server of the
temporary installation node. Now that Ceph cluster is available, the
configuration can be migrated to the S3 backend of Ceph RadosGW.

Edit the cluster specific layer of the hiera repository
`$ADMIN/hpc-privatedata/hieradata/$CLUSTER/cluster.yaml` to make the
`hpc-config` utilities push and download configuration in dedicated Ceph S3
bucket:

[source,yaml]
----
hpcconfig::apply::config_options:
  DEFAULT:
    source:
      value: "http://s3-system.service.%{hiera('virtual_domain')}:7480/hpc-config"
    keys_source:
      value: "http://secret.service.%{hiera('virtual_domain')}:%{hiera('secret_port')}"
hpcconfig::push::config_options:
  global:
    cluster:     "%{hiera('cluster_name')}"
    mode:        's3'
  s3:
    access_key:  "%{hiera('s3::access_key')}"
    secret_key:  "%{hiera('s3::secret_key')}"
    bucket_name: 's3-system'
    host:        'rgw.service.virtual'
    port:        '7480'

s3::access_key: DEC::PKCS7[<access_key>]!
s3::secret_key: DEC::PKCS7[<secret_key>]!

s3cmd::config_options:
  default:
    access_key:  "%{hiera('s3::access_key')}"
    secret_key:  "%{hiera('s3::secret_key')}"
    host_bucket: '%(bucket)s.service.virtual:7480'
    host_base:   'rgw.service.virtual:7480'
----

Push and apply configuration:

----
# hpc-config-push
# clush -bg service hpc-config-apply -v
----

At this stage, the `hpc-config` utilities are configured to use Ceph. Try this
new configuration by running them once again:

----
# hpc-config-push
# clush -bg service hpc-config-apply -v
----

Starting from this moment, all the cluster configuration is hosted in the Ceph
distributed filesystem.
