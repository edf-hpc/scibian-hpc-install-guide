= Installation procedures

////
TODO: fill this chapter to document all the steps to follow in order to install
a Scibian HPC cluster.
////

== Internal repository

////
TODO: document the steps to initialize the internal hieradata repository
////

== Cluster definition

////
TODO: document how-to write cluster definition in hiera, with reference to
[Puppet-HPC reference documentation]
////

This section guides you through the steps to define your cluster in Hiera. 
More detailed information can be found in 
https://github.com/edf-hpc/puppet-hpc/blob/master/doc/src/arch.asc#cluster-definition[the 
Puppet-HPC reference documentation, Software Architecture, Cluster Definition].

=== Global description of the cluster

First, the main shared parameters defining your cluster must be specified. This
is usually done in a file named `cluster.yaml`.

```
## Global description of the cluster
cluster_name:      'generic'                        # Name of the cluster
cluster_prefix:    'gen'                            # Prefix used in node name  
private_files_dir: "/admin/production/latest/files" # Where configuration files are stored
domain:            'generic.hpc.domain.org'         # Domain name used accross 
                                                    # all the machines
user_groups:                                        # Array of user groups 
  - "cl-%{hiera('cluster_name')}-users-dep1"        # allowed to access to the
  - "cl-%{hiera('cluster_name')}-users-dep2"        # cluster
  - "cl-%{hiera('cluster_name')}-users-dep2"
cluster_decrypt_password: 
                    'my_encrypted_password'         # Password encrypted with e-yaml
```                    

=== Cluster networks 

Once these parameters have been defined, the different networks used in the
cluster must be described. The 2 files containing the network details are
`cluster.yaml` and `network.yaml`.

==== In cluster.yaml

===== Topology

The network topology is usually specified in the file `cluster.yaml`. 
Typically, a cluster will have :

* some nodes connected to the enterprise WAN,
* a local administration network,
* a local low latency network efficient for calculations,
* a local management network including a subnet dedicated to Board Management Cards (bmc).

===== Bonding

If bonding is used between some network interfaces on some nodes of the 
cluster, it must be defined in `cluster.yaml`. The `network::bonding_options` 
should be used to specify the options for each bond.

===== Bridges

In the same way, it is also possible to set up bridge interfaces. 
The specification should be done by defining the `network::bridge_options`.

===== Example

A full example of network definition in `cluster.yaml` is detailed in
https://github.com/edf-hpc/puppet-hpc/blob/master/doc/src/arch.asc#network-definitions[the section Network Definitions of the Puppet-HPC Reference].

==== In network.yaml

The file `network.yaml` includes a hash called `master_network` that lists a 
description of network settings for all the nodes in the cluster. A detailed 
example is available in
https://github.com/edf-hpc/puppet-hpc/blob/master/doc/src/arch.asc#node-definitions[the section Node Definitions of the Puppet-HPC Reference].

== Generate keys

////
TODO: document the steps to generate the keys, certificates, and so on.
////
 
== Install admin node

////
TODO: document the steps to boot the admin node with rikenter and
install/configure minimal installation system in live.
////

== Install generic servers

////
TODO: document how-to install and configure generic  nodes.
////

== Configure Ceph

////
TODO: document how-to deploy and configure Ceph cluster.
////

=== Service initialization

==== mon init 

This procedure only works on a running cluster, the initial mon creation
uses another command.

From an *admin* node:

--------
# cd <ceph deploy directory>
# ceph-deploy --overwrite-conf mon create <mon hostname>
--------

==== osd

==== mds

MDS has no state on the node, the configuration in puppet is sufficient
to initialize the service.

==== radosgw

RadosGW has no state on the node, the configuration in puppet is sufficient
to initialize the service.

== Re-install admin node

////
TODO: document how-to install admin node with generic service nodes.
////

== Install virtual machines

////
TODO: document how-to install and configure virtual machines with references to
[bootstraping guides] for service.
////

== Build diskless image

Diskless nodes use squashfs images downloaded at boot. A simple way to
generate these images is to use https://github.com/edf-hpc/clara[`clara`].
Once configured, `clara image create` can be used to build Debian image(s).

Clara can install a selected list of packages into the image, manage
the repositories used and add specific files and directories.

Example:
-------------------------------------------------------------------------------
# clara image create scibian8
-------------------------------------------------------------------------------

Clara can also generate an `initrd` (initial ramdisk) to use with the image.
It can download the image either by http or bitorrent protocol.

Example:
-------------------------------------------------------------------------------
# clara image initrd scibian8
-------------------------------------------------------------------------------

Initrd must be regenerated each time the kernel image is modified.

See the http://edf-hpc.github.io/clara/[clara documentation]  for a full
list of options and possibilities.

Every time the image and the initrd are regenerated, they must be made
available to the deployment system.

== Boot nodes

To remotely start the nodes, you can use https://github.com/edf-hpc/clara[`clara`].
The ipmi plugin from clara is used for powering on/off selected nodes,
managing the boot node mode (if you want to install OS on disk or load it in RAM)
and accessing to the remote console.

See the http://edf-hpc.github.io/clara/[clara documentation] for a full
list of options and possibilities.

The commands from the following example, show how to reboot a node with
the boot menu, choose the operating system and boot mode between "diskless"
or "on disk".

Example:
-------------------------------------------------------------------------------
# clara ipmi pxe <node>
# clara ipmi reboot <node>
# clara ipmi connect <node>
-------------------------------------------------------------------------------

After the last command, when the login prompt is shown, you can connect on node
as root and monitor the progress of the puppet configuration by running
the following command:

-------------------------------------------------------------------------------
# journalctl -f -u hpc-config-apply
-------------------------------------------------------------------------------

When the network is up in the node you can also disconnect from the console
(press on "!" and "." keys as explained on the clara documentation) and
connect to the node via ssh.
