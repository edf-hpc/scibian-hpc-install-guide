== Boot sequence

=== Initial common steps

The servers of the cluster can boot on their hard disks or via network, using
the PXE protocol. In normal operations, all service nodes are installed on 
hard disks, and all *user* nodes (compute and user frontends) uses the
network method to boot the diskless image.
A service node can use the PXE method when it is being installed.
The boot sequence between the power on of the node and the boot of the initrd
is identical regardless of the system booted (installer or diskless image).

The steps are described by the diagram below :

image::src/img/boot_sequence.svg[image]

When a node boots on its network device, after a few (but generally
time-consuming) internal checks, it loads and runs the PXE ROM stored inside the
Ethernet adapter. This ROM first sends a DHCP request to get an IP address and
other network parameters. The DHCP server gives it an IP address alongside the
filename parameter. This filename is the file the PXE ROM is supposed to
download using TFTP protocol. Unfortunately, the PXE ROM commonly available in
Ethernet adapters only supports this network procotol which is rather limited and
unreliable.

On Scibian supercomputers, the value of this filename parameter is defined by
puppet roles in the puppet profile `bootsystem`, and therefore usually in Hiera
in the `profiles::bootsystem::boot_params` hash. It is set to launch the open
source iPXE software, because it delivers many powerful features such as HTTP
protocol support. This way, it is used as a workaround to hardware PXE ROM
limitations.

The virtual machines boots like any other node, except QEMU uses iPXE as the
PXE implementation for its virtual network adapters. This means that the
virtual machines go directly to this step.

The iPXE bootloader must perform another DHCP request since the IP settings are lost
when the bootloader is loaded. The DHCP server is able to recognize this request
originates from an iPXE ROM. In this case, it sets the filename parameter with
an HTTP URL to a CGI written in Python: `bootmenu.py`. If the DHCP server already knows 
the originating node and its MAC address (with a statically assigned IP address),
it also sends the hostname in the answer. Otherwise, the DHCP request is not
honored.

Then, the iPXE bootloader sends the GET HTTP request to this URL. In this request, it
also adds in parameters its hostname as it was given by the DHCP server.

On HTTP server side, the Python script bootmenu.py is run as a CGI program. This
script parses his configuration file `/etc/hpc-config/bootmenu.yaml` to get
the parameters to properly boot the node: serial console and ethernet device
to use, default boot mode (diskless, installer, etc ..). Then it generates an 
iPXE profile with a menu containing all possible boot entries. Finally, a 
timeout parameter is added to the iPXE profile.

The iPXE bootloader downloads and loads this dynamically generated profile.
Without any action from the administrator, iPXE waits the timeout and loads the
default entry set by the Python script.

NOTE: If one of the following conditions is satisfied:
* the hostname parameter is empty
* the node could not be found in `/etc/hpc-config/bootmenu.yaml` file
Then, the defaults choices from the config file are used.

=== Disk installation

Here is the sequence diagram of a Scibian server installation on disk, right
after PXE boot common steps:

image::src/img/boot_sequence_diskinstallation.svg[image]

The iPXE ROM downloads the Linux kernel and the initrd archive associated with
the boot menu entry. The kernel is then run with all the parameters given
in the menu entry, notably with the HTTP url to the preseed file.

The initrd archive contains the debian installer program. This program starts
by sending a new DHCP request to get an IP address. Then, it downloads the
preseed file located at the URL found in the `url ` kernel parameter. This 
preseed file contains all the answers of the questions asked by debian 
installer program. This way, the installation process is totally automated and
does not require any interaction from the administrator.

During the installation, many Debian packages are retrieved from Debian 
repositories.

At the end of the installation, debian installer runs the commands set in
`late_command` parameter of the preseed file. On Scibian supercomputers, this
parameter is used to run the following steps:

* Download though HTTP the hpc-config-apply configuration files,
* Run `hpc-config-apply` inside the chroot environment of the newly installed 
system.

Detailed functionning of the `hpc-config-apply` script is not described here,
but it involves downloading and installation of other debian packages 
(according to the node role), execution of various softwares and writing of
various files on the installed system.
Please refer to https://github.com/edf-hpc/puppet-hpc/blob/master/doc/manpages/hpc-config-apply.md[`hpc-config-apply(1)` man page]
for full usage documentation.

Finally, when the execution of the commands are over, the server
reboots.

Once the servers are installed, they are configured through IPMI with Clara
to boot on their disk devices first. Please refer to Clara documentation
for further details.

=== Diskless boot

Here is the sequence diagram of diskless nodes boot process, right after PXE
boot common steps:

image::src/img/boot_sequence_disklessboot.svg[image]

iPXE bootloader downloads the Linux kernel and the initrd image defined within the
default boot menu entry and runs them with the provided parameters. Among these
parameters, there are notably:

* `fetch` whose value is an HTTP URL to a torrent file available on the HTTP
  server of the supercomputer,
* `cowsize` whose value is the size of the ramfs filesystem mounted on
  _/lib/live/mount/overlay_,
* `disk_format` if this parameter is present the device indicated is formatted
on node boot,
* `disk_raid` if this parameter is present a software raid is created with the
parameters indicated on node boot.

Within the initrd images, there are several specific scripts that come from 
`live-boot`, `live-torrent` and specific Scibian Debian packages. Please refer
to the following sub-section <<Architecture,Advanced Topics, Generating diskless initrd>> 
for all explanations about how these scripts have been added to to the 
initramfs image.

These scripts download the torrent file at the URL in `fetch` parameter, 
launches the `ctorrent` Bittorrent client. This client extracts from this file
the IP address of the Bittorrent trackers and the files to download using
Bittorrent protocol.
There is actually one file to download, the SquashFS image, which the client will
download in P2P mode by gathering small chunks on several other nodes. Then, once
the file has been fully retrieved, the image is mounted after executing tasks
like formatting disk or setting a raid array if indicated in the kernel options
passed by the boot menu.
Then, the real init system is started and it launches all system services. One
of these services is `hpc-config-apply.service` which runs the 
`hpc-config-apply` script.

Detailed functionning of the `hpc-config-apply` script is not described here,
but it involves downloading and installation of other debian packages 
(according to the node role), execution of various softwares and writing of
various files on the installed system.
Please refer to https://github.com/edf-hpc/puppet-hpc/blob/master/doc/manpages/hpc-config-apply.md[`hpc-config-apply(1)` man page]
for full usage documentation.

Finally, the node is ready for production.
