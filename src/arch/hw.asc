== Hardware architecture

The following diagram represents the reference high-level hardware architecture
of Scibian HPC clusters:

[[img-hw_arch]]
.Scibian HPC cluster hardware reference architecture
image::src/img/arch_hw.svg[width=700]

=== Networks

The cluster is connected to three physically separated networks:

* The *WAN network*, an Ethernet based network with L3 network routers which
  connect the IP networks of the HPC cluster to the organization network.

* The *low-latency network* for both I/O to the storage system and distributed
  computing communications (typically MPI messages) between compute nodes. The
  hardware technologies of this network may vary upon performance requirements
  but it generally involves high bandwidth (10+GB/s) and low latency technologies
  such as InfiniBand, Omni-Path or 10GB Ethernet.

* The *administration network* used for basically every other internal network
  communications: deployment, services, administrator operations, etc. It must
  be a L2 Ethernet network with dedicated switches.

It is recommended to split the administration Ethernet network with a VLAN
dedicated to all management devices (BMC footnote:[Baseboard Management Card],
CMC footnote:[Chassis Management Card], etc). This has significant advantages:

* It significantly reduces the size of Ethernet broadcast domains which notably
  increases DHCP reliability and drops Ethernet switches load.
* It slightly increases security since the IP access to the management devices can
  be restricted to nodes accessing the VLAN or by a firewall on an IP router.

=== Administration cluster

The administration cluster is composed by two types of nodes: the *admin node*
and the *generic service nodes*.

The *admin node* is the access node for administrators and the central point of
administrative operations. All common administrative actions are performed on
this node. It does not run any intensive workloads, just simple short-lived
programs and it does not need to be very powerful. It does not store sensible data
nor run critical services, so it does not need to be very reliable either. Example
of hardware specifications:

[cols="1,3a"]
|===

|CPU
|1 x 4 cores

|RAM
|8GB ECC

|Network
|* 1 x 1GB bonding on administration network
* 1 x 1GB bonding on WAN network
* 1 link on low-latency network

|Storage
|2 x 300GB RAID1 SATA hard disk

|PSU
|Non-redundant

|===

The *generic service nodes* run all critical infrastructure services (within
service virtual machines) and manage all production administrative data.
Scibian HPC requires a pool from 3 (minimum) to 5 (recommended) generic service
nodes. The pool works in active cluster mode, the load is balanced with
automatic fail-over. All generic service nodes of a cluster must be fairly
identical for efficient load-balancing.

The generic service nodes manage the production data into a distributed
object-storage system. It is highly recommended that the nodes have a dedicated
block storage device for this purpose. The workload is mostly proportional to
the number of compute nodes but the generic service nodes must be quite powerful
to comfortably handle load peaks happening during some operations (**ex:** full
cluster reboot). Also, since services are run into virtual machines, a
fairly large amount of RAM is required. Services can generate a lot of
traffic on the administration network, it is relevant to provide a network
adapter with high bandwidth. Even though high-availability is ensured
at the software level with automatic fail-over between generic service nodes, it
is nevertheless recommended to get hardware redundancy on most devices of the
generic service nodes to avoid always risky and hazardous service migrations as
much as possible. Example of hardware specifications:

[cols="1,3a"]
|===

|CPU
|2 x 16 cores

|RAM
|64GB ECC

|Network
|* 2 x 10GB bonding on administration network
* 2 x 1GB bonding on WAN network
* 1 link on low-latency network

|Storage
|* 2 x 300GB RAID1 SATA hard disk for host
* 2 x 1TB SSD SAS or NVMe PCIe for object-storage system

|PSU
|Redundant

|===

All physical nodes must be connected to all three physical networks. There are
virtual bridges on the host of the generic service nodes connected to the WAN,
administration (and eventually management) networks. The service virtual
machines have connections to the virtual bridges upon their hosted service
requirements.

=== User-space cluster

The user-space cluster is composed of *frontend nodes* and *compute nodes*.

The nodes of the user-space cluster are deployed with a diskless live system
stored in RAM. It implies that, technically speaking, the nodes do not
necessarily need to have local block storage devices.

The *frontend nodes* are the access hosts for users so they must be connected to
all three physical networks. It is possible to have multiple frontend nodes in
active cluster mode for load-balancing and automatic fail-over. The exact
hardware specifications of the frontend nodes mostly depend on user needs and
expectations. Users may need to transfer large amount of data to the cluster, it
is therefore recommended to provide high-bandwidth network adapters for the WAN
network. These nodes can also be designed to compile computational codes and 
in this case, they must be powerful in terms of CPU, RAM and local storage I/O.

The *compute nodes* run the jobs so they must provide high performances. Their
exact hardware specifications totally depend on user needs. They must be
connected to both the administration and the low-latency networks.

=== Storage system

The storage system is designed to host user data. It provides one or
several shared POSIX filesystems. The evolved storage technologies depend
on user needs ranging from a simple NFS NAS to a complex distributed
filesystem such as Lustre or GPFS with many SAN and I/O servers.
