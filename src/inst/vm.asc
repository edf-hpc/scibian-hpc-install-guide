== Service virtual machines

On Scibian HPC clusters, the additional services are hosted inside virtual
machines for more flexibility and better resources partitionning. These service
virtual machines run on the generic service nodes. On the generic services
nodes, the virtual machines are managed by Libvirt service. The ditributed
instances of Libvirt are controlled centrally from the admin node with Clara
utility. The following sub-sections explain how to setup these software
components.

=== Libvirt settings

The Libvirt service must create various virtual networks to connect the virtual
machines to the HPC cluster and a storage pool on Ceph RDB interface to store
the virtual disks of the virtual machines. These virtual resources are setup
with the following configuration in the cluster specific layer of the hiera
repository:

[source,yaml]
----
virt_ceph_uuid: '<uuid>'

profiles::virt::networks:
  'administration':
    'mode': 'bridge'
    'interface': 'br0'
  'management':
    'mode': 'bridge'
    'interface': 'br1'
  'wan':
    'mode': 'bridge'
    'interface': 'br2'

profiles::virt::secrets:
  'client.libvirt':
    'type':  'ceph'
    'uuid':  "%{hiera('virt_ceph_uuid')}"
    'value': DEC::PKCS7[<key>]!

profiles::virt::pools:
  'rbd-pool':
    'type': 'rbd'
    'hosts':
      - 'fbservice2'
      - 'fbservice3'
      - 'fbservice4'
    'auth':
      'type':     'ceph'
      'username': 'libvirt'
      'uuid':     "%{hiera('virt_ceph_uuid')}"
----

In this example, the following values must be replaced:

* `<key>` is given by the following command:
----
# ceph auth get-key client.libvirt`
----
* `<uuid>` is an arbitrary UUIDfootnote:[Universally Unique IDentifier, a
  128-bit number used to identify information in computer systems] to identify
  uniquely the secret. For example, it can be generated with this command:
----
# python  -c 'import uuid; print uuid.uuid1()'
----

The profile `profiles::virt::host` must be added to service nodes role
definition.

Push and apply configuration on the generic service nodes:

----
# hpc-config-push && clush -bg service hpc-config-apply
----

=== Clara configuration

Clara has dedicated configuration for its _virt_ plugin. This configuration is
set with the following two hashes in the cluster specific layer of the hiera
repository:

[source,yaml]
----
clara::virt_options:
  'nodegroup:default':
    'default':                  'true'
    'nodes':                    'fbservice1,fbservice2,fbservice3,fbservice4'
  'pool:default':
    'default':                  'false'
  'pool:rbd-pool':
    'default':                  'true'
    'vol_pattern':              '{vm_name}_{vol_role}'
  'template:default':
    'default':                  'true'
    'xml':                      'domain_default_template.xml'
    'vol_roles':                'system'
    'vol_role_system_capacity': '60000000000'
    'networks':                 'administration'

clara::virt_tpl_hpc_files:
  '/etc/clara/templates/vm/domain_default_template.xml':
    source: "%{::private_files_dir}/virt/domain_default_template.xml"
----

The `clara::virt_options` hash notably specifies the list of generic services
nodes that hosts the virtual machines and the domain templates and parameters
associated to each service virtual machine. For the moment, only the default
domain template and parameters are set. The second hash
`clara::virt_tpl_hpc_files` defines the templates of Libvirt XML domains
definitions. In this example, there is one default domain XML template for all
virtual machines which should be fine for most Scibian HPC clusters.

The domain XML template must be located in
`$ADMIN/hpc-privatedata/files/$CLUSTER/virt/domain_default_template.xml`. Here
is a full example of this file:

[source,xml]
----
include::../examples/default_domain_template.xml[]
----

In this example, the following values must be replaced:

* `<ip_mon_server_*>` are the static IP addresses of the Ceph MON servers on the
  administration network.
* `<uuid>` is the UUID for Libvirt Ceph RBD secret generated in the previous
  sub-section.

Deploy these new settings by pushing and applying the configuration on the admin
node:

----
# hpc-config-push && hpc-config-apply -v
----

=== Virtual machine definitions

Now that Libvirt and Clara virt plugin are properly setup, the various service
virtual machines can be defined. The steps to define the service virtual
machines are mostly generic and common to all of them. As an example for this
documentation, the two service virtual machines `fbdoe[1-2]` will be defined.

The first step is to define the `boot_params` of the virtual machines in
`$ADMIN/hpc-privatedata/hieradata/$CLUSTER/cluster.yaml`:

----
boot_params:
  [...]
  fbdoe[1-2]:
    boot_os:            'scibian9_disk'
    ipxebin:            'ipxe_noserial.bin'
----

Eventually, in the same file, an additional domain template and parameters
association can be appended to the `clara::virt_options` for these new virtual
machines, if the default domain parameters are not appropriate:

----
clara::virt_options:
  [...]
  'template:proxy':
    'vm_names':                 'fbdoe[1-2]'
    'xml':                      'domain_default_template.xml'
    'vol_roles':                'system'
    'vol_role_system_capacity': '60000000000'
    'networks':                 'administration,wan'
    'core_count':               '16'
    'memory_kib':               '16777216'
----

In this example, the following settings are overriden from the defaults:

* the virtual block storage device has a size of 60GB,
* 2 network devices attached to the _administration_ and _wan_ networks,
* 16 virtual CPU cores,
* 16GB of RAM.

Then, the new role _doe_ must be defined in file
`$ADMIN/hpc-privatedata/hieradata/$CLUSTER/roles/doe.yaml` with all the
appropriate profiles.

Push and apply configuration on admin node:

----
# hpc-config-push && hpc-config-apply -v
----

Extract MAC address of the virtual machine on the administration network:

----
# clara virt getmacs <VM>
----

Then add the network settings of the virtual machines in the `master_network`
hash with their MAC addresses:

[source,yaml]
----
master_network:
  fbdoe1:
    fqdn: "fbdoe1.%{hiera('domain')}"
    networks:
      administration:
        'DHCP_MAC': 'aa:bb:cc:dd:ee:0a'
        'IP':       '10.1.0.11'
        'device':   'eth0'
        'hostname': 'fbdoe1'
      wan:
        'IP':       '10.3.0.11'
        'device':   'eth1'
        'hostname': 'wanfbdoe1'
  fbdoe2:
    fqdn: "fbdoe2.%{hiera('domain')}"
    networks:
      administration:
        'DHCP_MAC': 'aa:bb:cc:dd:ee:0b'
        'IP':       '10.1.0.12'
        'device':   'eth0'
        'hostname': 'fbdoe2'
      wan:
        'IP':       '10.3.0.12'
        'device':   'eth1'
        'hostname': 'wanfbdoe2'
----

Eventually, virtual IP addresses can also be defined for the virtual machines in
the `vips` hash of the same file.

Generate the SSH host keys in synchronization with the `master_network`:

----
# puppet-hpc/scripts/sync-ssh-hostkeys.sh hpc-privatedata $CLUSTER
----

Push and apply the new configuration on the generic service nodes:

----
# hpc-config-push && clush -bg service hpc-config-apply -v
----

Define the new virtual machines with Clara on the generic service node of your
choice, for example `fbservice1`:

----
# clara virt define fbdoe[1-2] --host=fbservice1
----

NOTE: The choice of the generic service node is not critical as the service
virtual machines can be migrated from one generic service node to another at
any time.

Then start the virtual machine by wiping its virtual block storage devices and
boot in PXE mode:

----
# clara virt start fbdoe[1-2] --wipe
----

Eventually, watch the serial console with:

----
# ssh -t fbservice1 -- virsh console fbdoe1
----

=== Required virtual machines

You are free to define the service virtual machines you want on Scibian HPC
clusters. The service virtual machines can run any software services you would
like. However, some specific generic virtual machines are required in the
reference architecture to run some mandatory additional services.

The required service virtual machines are:

* two *proxy* virtual machines with the `auth::replica` profile for managing the
  LDAP directory replica. The installation of the LDAP directory replica of the
  _proxy_ nodes is documented in the <<inst-auth-replica,Directory replica
  sub-section>> of the LDAP Authentication_ section of this installation
  procedure.
* two  *batch* virtual machines with the `jobsched::server` and `db::server`
  profiles for Slurm controller, SlurmDBD accounting service and MariaDB galera
  database. The installation of the Slurm server-side components on the _batch_
  nodes is documented in the <<inst-slurm,Slurm section>> of this installation
  guide.
* two *p2p* virtual machines with the `p2p::seeder`, `p2p::tracker` and
  `http::diskless` profiles for serving files to boot diskless nodes with
  Bittorrent. The installation of the _p2p_ nodes is pretty straightforward as
  long as the required profiles are enabled. The creation of the diskless
  environment is documented in the <<inst-nodes-diskless,Build diskless image
  sub-section>> of the _Frontend and compute nodes_ section of the installation
  procedure.
