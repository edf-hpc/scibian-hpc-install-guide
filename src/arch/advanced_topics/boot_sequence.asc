=== Boot sequence

==== Initial common steps

The servers of the cluster can boot on their hard disks or via the network, 
using the PXE protocol. In normal operations, all service nodes are installed 
on hard disks, and all nodes of the userspace (*compute* and *frontend* nodes) 
use the network method to boot the diskless image.
A service node can use the PXE method when it is being installed.
The boot sequence between the `power on` event on the node and the boot of the 
initrd is identical regardless of the system booted (installer or diskless 
image).

The steps of the boot sequence are described on the diagram below:

image::src/img/boot_sequence.svg[image]

When a node boots on its network device, after a few (but generally
time-consuming) internal checks, it loads and runs the PXE ROM stored inside the
Ethernet adapter. This ROM first sends a DHCP request to get an IP address and
other network parameters. The DHCP server gives it an IP address alongside the
filename parameter. This filename is the file the PXE ROM downloads using the 
TFTP protocol. This protocol, which is rather limited and unreliable is used 
here because the PXE ROM commonly available in Ethernet adapters only supports 
this network procotol.

The file to download depends on the type of nodes or roles. On Scibian HPC 
clusters when using the Puppet-HPC software stack, the required filename for 
the current node is set in the profile `bootsystem` and therefore its value is 
usually specified in Hiera in the `profiles::bootsystem::boot_params` hash. 
It is set to launch the open source iPXE software, because it delivers many 
powerful features such as HTTP protocol support. This way, it is used as a 
workaround to hardware PXE ROM limitations.

The virtual machines boot like any other node, except QEMU uses iPXE as the
PXE implementation for its virtual network adapters. This means that the
virtual machines go directly to this step.

The iPXE bootloader must perform another DHCP request since the IP settings are lost
when the bootloader is loaded. The DHCP server is able to recognize this request
originates from an iPXE ROM. In this case, it sets the filename parameter with
an HTTP URL to a CGI written in Python: __bootmenu.py__. If the DHCP server already knows 
the originating node and its MAC address (with a statically assigned IP address),
it also sends the hostname in the answer. Otherwise, the DHCP request is not
honored.

Then, the iPXE bootloader sends the GET HTTP request to this URL. In this request, it
also adds to the parameters its hostname as it was given by the DHCP server.

On the HTTP server side, the Python script `bootmenu.py` is run as a CGI program. This
script parses its configuration file __/etc/hpc-config/bootmenu.yaml__ to get
the parameters to properly boot the node: serial console and ethernet device
to use, default boot mode (diskless, installer, etc ..). Then it generates an 
iPXE profile with a menu containing all possible boot entries. Finally, a 
timeout parameter is added to the iPXE profile.

The iPXE bootloader downloads and loads this dynamically generated profile.
Without any action from the administrator, iPXE waits the timeout and loads the
default entry set by the Python script.

NOTE: If one of the following conditions is satisfied: either the hostname parameter is empty, or the node could not be found in the 
__/etc/hpc-config/bootmenu.yaml__ file, then, the default choices from the config file are used.

==== Disk installation

Here is the sequence diagram of a Scibian server installation on disk, right
after the PXE boot common steps:

image::src/img/boot_sequence_diskinstallation.svg[image]

The iPXE ROM downloads the Linux kernel and the initrd archive associated with
the boot menu entry. The kernel is then run with all the parameters given
in the menu entry, notably with the HTTP url to the preseed file.

The initrd archive contains the Debian Installer program. This program starts
by sending a new DHCP request to get an IP address. Then, it downloads the
preseed file located at the URL found in the `url ` kernel parameter. This 
preseed file contains all the answers to the questions asked by the Debian 
Installer program. This way, the installation process is totally automated and
does not require any interaction from the administrator.

During the installation, many Debian packages are retrieved from Debian 
repositories.

At the end of the installation, Debian Installer runs the commands set in the
`late_command` parameter of the preseed file. On Scibian HPC clusters, this
parameter is used to run the following steps:

* Download through HTTP the __hpc-config-apply__ script,
* Run __hpc-config-apply__ inside the chroot environment of the newly installed 
system.

Detailed functionning of the __hpc-config-apply__ script is not described here,
but it involves:

* downloading and installing additional Debian packages depending on the node role, 
* executing various types of software 
* and writing various configuration files on the installed system.

Please refer to https://github.com/edf-hpc/puppet-hpc/blob/master/doc/manpages/hpc-config-apply.md[`hpc-config-apply(1)` man page]
for a full documentation on how to use this script.

Finally, when the execution of the commands are over, the server
reboots.

Once the servers are installed, they are configured through IPMI with Clara
to boot on their disk devices first. Please refer to Clara documentation
for further details.

==== Diskless boot

Here is the sequence diagram of the boot process for diskless nodes, right after
the PXE boot common steps:

image::src/img/boot_sequence_disklessboot.svg[image]

The iPXE bootloader downloads the Linux kernel and the initrd image defined within the
default boot menu entry and runs them with the provided parameters. Among these
parameters, there are notably:

* `fetch` whose value is an HTTP URL to a torrent file available on the HTTP
  server of the supercomputer,
* `cowsize` whose value is the size of the ramfs filesystem mounted on
  _/lib/live/mount/overlay_,
* `disk_format` if this parameter is present the device indicated is formatted
on node boot,
* `disk_raid` if this parameter is present a software raid is created with the
parameters indicated on node boot.

Within the initrd images, there are several specific scripts that come from 
`live-boot`, `live-torrent` and specific Scibian Debian packages. Please refer
to the following sub-section <<Architecture,Advanced Topics, Generating diskless initrd>> 
for all explanations about how these scripts have been added to the 
initramfs image.

These scripts download the torrent file at the URL specified in the `fetch` 
parameter, then they launch the `ctorrent` Bittorrent client. This client 
extracts from the torrent file the IP address of the Bittorrent trackers and 
the names of the files to download using the Bittorrent protocol.
There is actually one file to download, the SquashFS image, that the client 
will download in P2P mode by gathering small chunks on several other nodes. 
Then, once the file has been fully retrieved, the image is mounted after 
executing some preliminary tasks like formatting the disk or setting up a raid
array if it has been indicated in the kernel options passed by the boot menu.
Then, the real init system is started and it launches all the system services. 
One of these services is `hpc-config-apply.service` which runs the 
__hpc-config-apply__ script.

As for the part regarding the installation with a disk, how the __hpc-config-apply__ script works is not described here.
Please refer to https://github.com/edf-hpc/puppet-hpc/blob/master/doc/manpages/hpc-config-apply.md[`hpc-config-apply(1)` man page]
for a full documentation on this topic.

Finally, the node is ready for production.
