== Hardware architecture

The following diagram represents the reference high-level hardware architecture
of Scibian HPC clusters:

[[img-hw_arch]]
.Scibian HPC cluster hardware reference architecture
image::src/img/arch_hw.pdf[width=700]

=== Networks

The cluster is connected to three physically separated networks:

* The *WAN network*, an Ethernet based network with L3 network routers which
  connect the IP networks of the HPC cluster to the organisation network.

* The *lowlatency network* for both I/O to the storage system and distributed
  computing communications (typically MPI messages) between compute nodes. The
  hardware technologies of this network may vary upons performances requirements
  but it generally involves high bandwith (10+GB/s) and low latency technologies
  such as Infiniband, Omni-Path or 10GB Ethernet.

* The *administration network* used for basically every other internal network
  communications: deployment, services, administrator operations, etc. It must
  be a L2 Ethernet network with dedicated switches.

Is is recommended to split the administration Ethernet network with a VLAN
dedicated to all management devices (BMC footnote:[Baseboard Management Card],
CMC footnote:[Chassis Management Card], etc). This has significant advantages:

* Significantly reduces the size of Ethernet broadcast domains which notably
  increases DHCP reliability and drops Ethernet switches load.
* Slightly increases security since the IP access to the management devices can
  be restricted to nodes accessing the VLAN or by a firewall on a IP router.

=== Administration cluster

The administration cluster is composed by two types of nodes: the *admin node*
and the *generic service nodes*.

The *admin node* is the administrators access node and the central point of
administrative operations. All common administrative actions are performed on
this node. It does not run any intensive workloads, just simple short-lived
programs, it does not need to be very powerful. It does not store sensible data
nor run critical services, so it does need to be very reliable neither. Example
of hardware specifications:

[cols="1,3a"]
|===

|CPU
|1 x 4 cores

|RAM
|8GB ECC

|Network
|* 1 x 1GB bonding on administration network
* 1 x 1GB bonding on WAN network
* 1 link on lowlatency network

|Storage
|2 x 300Go RAID1 SATA hard disk

|PSU
|Non-redundant

|===

The *generic service nodes* run all critical infrastructure services (within
services virtual machines) and manage all production administrative data.
Scibian-HPC requires a pool from 3 (minimum) to 5 (recommended) generic service
nodes. The pool works in active cluster mode, the load is balanced with
automatic failover. All generic service nodes of a cluster must be fairly
identical for efficient load-balancing.

The generic service nodes manage the production data into a distributed
object-storage system. It is highly recommended that the nodes have a dedicated
block storage device for this purpose. The workload is mostly proportional to
the number of compute nodes but the generic service nodes must be quite powerful
to comfortably handle load peaks happening during some operations (**ex:** full
cluster reboot). Also, since the services are run into virtual machines, a
fairly large amount of RAM is required. The services can generate a lot of
trafic on the administration network, it is relevant to provide a network
adapter with high bandwidth. Even though high-availability is ensured
at the software level with automatic failover between generic service node, it
is nevertheless recommended to get hardware redundancy on most devices of the
generic service nodes to avoid always risky and hazardous service migrations as
much as possible. Example of hardware specifications:

[cols="1,3a"]
|===

|CPU
|2 x 16 cores

|RAM
|64GB ECC

|Network
|* 2 x 10GB bonding on administration network
* 2 x 1GB bonding on WAN network
* 1 link on lowlatency network

|Storage
|* 2 x 300Go RAID1 SATA hard disk for host
* 2 x 1TB SSD SAS or NVMe PCIe for object-storage system

|PSU
|Redundant

|===

All physical nodes must be connected to all three physical networks. There are
virtual bridges on the host of the generic service nodes connected to the WAN,
administration (and eventually management) networks. The services virtual
machines have connections to the virtual bridges upon their hosted services
requirements.

=== Userspace cluster

The userspace cluster is composed of *frontend nodes* and *compute nodes*.

The nodes of the userspace cluster are deployed with a diskless live system
stored in RAM. It implies that, Technically speaking, the nodes do not
necessarily need to have local block storage devices.

The *frontend nodes* are the users access hosts so they must be connected to
all three physical networks. It is possible to have multiple frontend nodes in
active cluster mode for load-balancing and automatic failover. The exact
hardware specifications of the frontend nodes mostly depend on users needs and
expectations. Users may need to transfer large amount of data to the cluster, it
is therefore recommended to provide high-bandwidth network adapters for the WAN
network. These nodes can also be designed to compile compute codes and in this
case, they must be powerful in terms of CPU, RAM and local storage I/O.

The *compute nodes* run the jobs so they must provide high-performances. Their
exact hardware specifications totally depends on users needs. They must be
connected to both the administration and the lowlatency networks.

=== Storage system

The storage system is designed to host the users data. It provides one or
several shared POSIX filesystems. The evolved storage technologies depends
on users needs ranging from a simple NFS NAS to a complex distributed
filesystems such as Lustre or GPFS with many SAN and I/O servers.
