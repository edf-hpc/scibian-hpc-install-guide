= Production procedures

== MAC address change

This procedure explains how to modify the Puppet-HPC configuration to change an
hardware ethernet address after a motherboard replacement, for example.

=== Change the network.yaml file

First, the yaml file in the hieradata repository containing the
`master_network` hash must be edited to replace the old hardware address. A
description of this hash can be found in the Installation section of this guide.

=== Push the new configuration and apply it

The modified configuration must be pushed to the shared administration 
directory with the `hpc-config-push` command:

----
# hpc-config-push
INFO: creating archive /tmp/puppet-config-push/tmp_ndq0ujz/puppet-config-environment.tar.xz
INFO: S3 push: pushing data in bucket s3-system
----

Then apply the configuration on the `service` nodes, who runs the dhcp server:

----
# hpc-config-apply
----

NOTE: It is not possible to run the `hpc-config-apply` command on all the
service nodes at the same time exactly. A short delay must be respected as the
Ceph service can be disturbed by a restart of the network service.

== Password/keys changes

=== Root password

The hashed root password is stored in the variable
`profiles::cluster::root_password_hash` in yaml files. The value must be
encrypted using eyaml. It can be simply changed using the `eyaml` command.

 # eyaml edit cluster.yaml
 ...
 profiles::cluster::root_password_hash: DEC::PKCS7[hashed_password]!
 ...

Once changed, the new configuration must be applied on all the machines of the
cluster.

=== Root ssh key

The root ssh keys are stored in the internal repository. The privates keys must
be encrypted.
The ssh public rsa key is also in the variable
`openssh::server::root_public_key`.
It is necessary to change the files and the value of the variable at the same
time.
To avoid connections problems, it is necessary to follow these steps in this
order :
1. Change the keys files and the variable `openssh::server::root_public_key` in
the internal repository 
2. Apply the configuration on all the machines exept the *admin* one
3. Apply the new configuration on the *admin* server.

NOTE: In case of desynchronization between the keys on the *admin* node and
those on the others nodes, it is always possible to use the root password to
connect.

=== SSH host keys

The ssh host keys are stored, encrypted, in the internal repository.
To avoid connections problems, it is necessary to follow these steps in this
order :
1. Change the keys files in the internal repository 
2. Apply the configuration on all the machines of the cluster, including the
*admin* machine
3. Delete the file __/root/.ssh/known_hosts__ on the *admin* node.
4. When connecting to the nodes, __/root/.ssh/known_hosts__ will be
automatically populated if the Scibian HPC default configuration is
used.

=== Eyaml keys

Replacing the eyaml PKCS7 key pair consist in reality of two actions :
1. Generate a new pair of keys
2. Replace all the values encoded with the old pair with ones encoded with the
new pair of keys.

NOTE: As these operations implies decoding files and reencoding them with
another key pair, it is not possible to perform other administratives
operations (like applying the configuartyion on nodes) on the cluster in the
same time. The changing keys operation must be fully completed before resuming
"normal" administratives operations.

These steps must be followed in order to safely change the eyaml keys :

==== Save the old keys

 # cp /etc/puppet/secure/keys/private_key.pkcs7.pem /etc/puppet/secure/keys/private_key.pkcs7.pem.old
 # cp /etc/puppet/secure/keys/public_key.pkcs7.pem /etc/puppet/secure/keys/public_key.pkcs7.pem.old

==== Copy the new keys in __/etc/puppet/secure/keys/__

==== Decrypt all the yaml files encoded using the old keys :

 # eyaml decrypt --pkcs7-private-key /etc/puppet/secure/keys/private_key.pkcs7.pem.old --pkcs7-public-key /etc/puppet/secure/keys/public_key.pkcs7.pem.old hieradata/<cluster>/cluster.eyaml

==== Encrypt the files with the new keys :

 # eyaml encrypt hieradata/<cluster>/cluster.eyaml

It is not necessary to specify the paths of the keys if their files names does
not change.

==== Create a tarball, encode it with `clara enc` and add it to the __files__ directory of the internal repository :

 # tar cJf /tmp/keys.tar.xz /etc/puppet/secure/keys/private_key.pkcs7.pem /etc/puppet/secure/keys/private_key.pkcs7.pem
 # clara enc encode /tmp/keys.tar.xz
 # cp /tmp/keys.tar.xz.enc <internal repository>/files/<cluster>/eyaml
 # rm /tmp/keys.tar.xz /tmp/keys.tar.xz.enc

Where:

* <internal repository> is the directory that contains the clone of the internal
repository.
* <cluster> is the name of the cluster.

At this stage, the keys are now stored encrypted in the internal repository and
are available locally in the standard eyaml paths.

==== Apply the new configuration on the nodes where the propagation service runs :
In the default Scibian HPC configuration, the PKCS7 keys propagation service
runs on all the generic service nodes. Firstly the encoded tarball must be
manually copied on the nodes :

 # scp <internal repository>/files/<cluster>/eyaml/keys.tar.xz <generic server X>:/tmp

Where <generic server X> is the hostname of the generic server node.
Secondly apply the configuration using the new keys :

 # hpc-config-apply --keys-source=file:///tmp

This will copy the eyaml PKCS7 key pair in the right directory to be serviced
by the propagation service to all others nodes when applying the puppet
configuration.
These last two operations must be executed on all the generic service nodes.

==== Once tested, remove the old saved keys from the *admin* node :

 # rm /etc/puppet/secure/keys/private_key.pkcs7.pem.old /etc/puppet/secure/keys/public_key.pkcs7.pem.old

=== Internal repository encoding key

NOTE: As these operations implies decoding filds and reencoding them with
another key, it is not possible to perform other administratives operations
(like applying the configuartyion on nodes) on the cluster in the same time.
The changing key operation must be fully completed before resuming "normal"
administratives operations.

Replacing the AES key used to encode files in the internal repository consist in
several steps :

==== Generate a new AES key

 # openssl rand -base64 32

==== Replace all the encoded files in the internal repository by files encoded with the new key

For each encoded file in the internal repository, it is necessary to decode it
with the old key and reencode it with the new one.

 # clara enc decode <internal repository>/files/<cluster>/<filename>.enc
 # openssl aes-256-cbc -in <internal repository>/files/<cluster>/<filename> -out <filename>.enc -k <AES KEY>
 # rm <internal repository>/files/<cluster>/<filename>

Where:

* <internal repository> is the directory that contains the clone of the internal
repository
* <cluster> is the name of the cluster
* <filename> is the path of the file to encode 
* <AES KEY> is the random 256 bits key.

Use `clara` for both operations, decode and encode, is not possible as it
support only one AES key.

==== Replace the AES key in the Hiera repository

The AES key must be placed in __cluster_decrypt_password__ in the cluster layer of the
Hiera repository:

 # eyaml edit hieradata/<cluster>/cluster.eyaml

Replace the key:

 cluster_decrypt_password: DEC::PKCS7[<AES KEY>]!

Apply the new configuration on the *admin* node, to update `clara` configuration:

 # hpc-config-apply

=== Replication account password

The credentials used to replicate the external ldap tree are stored directly in
the ldif file used to configure the internal ldap server.

The steps to change these credentials are described here :

1. Decode the configuration ldif file

 # clara enc edit <internal repository>/files/<cluster>/<filename>.enc

2. The field to change is `olcSyncrepl:`, it contain all the necessary
informations to connect to the master ldap server (login, password, uri, etc ..)

3. Apply the new configuration on the *proxy* nodes.

4. Follow the LDAP bootstrap procedure as described in <<bootstrap-ldap, 
LDAP bootstrap>> on each *proxy* node. It is recommended to wait until the first
ldap replicate is complete before attempting to update the second, to not disrupt
authentification accross the cluster.

NOTE: It is possible to change others values with this procedure, for example 
the root ldap password.

=== Monitoring certificates

The certificates used for monitoring are stored, encrypted, in the internal
repository in __<internal repository>/files/<cluster>/icinga2/certs/__. Each
host has a certificate and a key.
The steps to follow to change them are :

1. Change the key and certificate files in the internal repository
2. Apply the configuration on the concerned node
3. Update the certificate on the icinga2 server

=== Munge key



=== Repo keyring

== Administration node re-installation

This procedure will wipe the first disk of the admin node, if some
customizations are not in the Puppet configuration, this should be handled
separately.

=== Power off

Before, powering off the administration node, check that:

- There is an alternative route to connect to the service node (can be the
  service nodes themselves)
- It is possible to connect to the BMC IPMI, and especially to the Serial Over
  LAN console
- It is possible to connect to the Ethernet administration network switch

The administration node has no critical service in the reference architecture,
so it can simply be powered off:

----
admin # poweroff
----

=== PXE setup

In some ethernet bonding setups, the node cannot do a PXE boot with an active
bonding configuration on the ethernet switch. If this is the case, refer to the
documentation of the network switch to disable the bonding configuration.

=== Power on

To be re-installed, the administration node must perform a network boot. This
can be configured with `ipmitool(1)` installed on a host that has access to the
BMC network interface:

----
# ipmitool -I lanplus -H <bmc host> -U <bmc username> -P chassis bootdev pxe
# ipmitool -I lanplus -H <bmc host> -U <bmc username> -P chassis power on
----

Next steps will happen once the node is installed and has rebooted, the
installation can be followed through serial console:
----
# ipmitool -I lanplus -H <bmc host> -U <bmc username> -P sol activate
----

=== Customization

After the node reboot, it is possible to connect to the node, and re-configure
customization made outside of the Puppet configuration (bash environment...).

=== Cleaning

If the ethernet switch configuration had to be modified to setup PXE boot, the
modification must be reverted to its nominal status.

== Service node re-installation

=== VMs migration

Before re-installing a Service node, active Virtual Machines on the nodes
should be migrated away from the node. Clara can be used to list the active VMs
and do the live migration.

Listing the VMs:

----
admin # clara virt list |grep clserviceX
----

Migrate the live VMs with the command:

----
admin # clara virt migrate <vmname> --dest-host clserviceY
----

=== Power off

These points should be checked before turning off a Service Node:

 * The ceph cluster should be `HEALTH_OK` (`ceph health`), with at least three
   OSD `in`
 * `consult` should return services as passing on at least three nodes
 * On an Intel OmniPath cluster, the `opafabricinfo` should return at least one
   Master and one Standby node

Once there is no VM remaining on the node, it can be powered off safely, the
other Service node should ensure there is no service outage.

The power off can be done from the node itself:

----
clserviceX # poweroff
----

=== PXE setup

In some ethernet bonding setups, the node cannot do a PXE boot with an active
bonding configuration on the ethernet switch. If this is the case, refer to the
documentation of the network switch to disable the bonding configuration.

=== Power on

To be re-installed, the service node must perform a network boot. This can be
configured with *clara*:

----
admin # clara ipmi pxe clserviceX
admin # clara ipmi on clserviceX
----

Next steps will happen once the node is installed and as rebooted, the
installation can be followed through serial console:
----
admin # clara ipmi connect clserviceX
----

=== Ceph

After a Service node re-installation, the ceph services: OSD, MDS and RadosGW
should be reconfigured automatically by the Puppet HPC configuration.

The Mon service (not present on every node), must be boot-strapped again. This
procedure is described with other <<bootstrap-ceph-mon, Ceph bootstrap
procedures>>.

=== Post-reboot checks

High-Speed network manager (Intel OmniPath):

----
admin # opafrabricinfo
----

The reinstalled node must appear as a *Master* or *Standby* node.

Check the ceph cluster is healthy:

----
admin # ceph status
----

The cluster should be `HEALTH_OK` with all OSDs, Mons and MDSs.

Consul:

----
admin # consul
----

All services on all nodes should have the state `passing`.

=== Cleaning

If the ethernet switch configuration had to be modified to setup PXE boot, the
modification must be reverted to its nominal status.
