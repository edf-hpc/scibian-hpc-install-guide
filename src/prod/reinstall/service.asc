== Service node re-installation

Before re-installing a Service node, active Virtual Machines on the nodes
should be migrated away from the node. Clara can be used to list the active VMs
and do the live migration.

Listing the VMs:

----
# clara virt list | grep clserviceX
----

Migrate the live VMs with the command:

----
# clara virt migrate <vmname> --dest-host clserviceY
----

These points should be checked before turning off a Service Node:

 * The ceph cluster should be `HEALTH_OK` (`ceph health`), with at least three
   OSD `in`
 * `consult` should return services as passing on at least three nodes
 * On an Intel Omni-Path cluster, the `opafabricinfo` should return at least one
   Master and one Standby node

Once there is no VM remaining on the node, it can be powered off safely, the
other Service node should ensure there is no service outage. The power off can
be done from the node itself:

----
# poweroff
----

NOTE: In some Ethernet bonding setups, the node cannot do a PXE boot with an
active bonding configuration on the Ethernet switch. If this is the case, refer
to the documentation of the network switch to disable the bonding configuration.

To be re-installed, the service node must perform a network boot. This can be
configured with *clara*:

----
# clara ipmi pxe clserviceX
# clara ipmi on clserviceX
----

Next steps will happen once the node is installed and as rebooted, the
installation can be followed through serial console:

----
# clara ipmi connect clserviceX
----

After a Service node re-installation, the ceph services: OSD, MDS and RadosGW
should be reconfigured automatically by the Puppet HPC configuration. The Mon
service (not present on every node), must be boot-strapped again. This procedure
is described with other <<bootstrap-ceph-mon, Ceph bootstrap procedures>>.

In order to validate the generic service node re-installation, there are some
relevant checks to perform.

* High-Speed network manager (Intel Omni-Path):

----
# opafrabricinfo
----

The reinstalled node must appear as a *Master* or *Standby* node.

* Check the ceph cluster is healthy:

----
# ceph status
----

The cluster should be `HEALTH_OK` with all OSDs, Mons and MDSs.

* Consul:

----
# consult
----

All services on all nodes should have the state `passing`.

NOTE: If the Ethernet switch configuration had to be modified to setup PXE boot,
the modification must be reverted to its nominal status.
