=== Consul and DNS integration

This diagram illustrates how Consul and the DNS servers integrate to provide
load-balanced and horizontally scaled network services with high-availability:

[[img-consul_dns_integ]]
.Consul, DNS server and services integration
image::src/img/arch_consul_dns_integ.svg[width=600]

The Consul agent daemon can run in two modes: server and client. The cluster of
Consul servers maintains the state of the cluster using the raft protocol. The
clients communicate with the servers to detect failures using the gossip
protocol. Both agents expose the data of the Consul cluster through a HTTP REST
API. On Scibian HPC clusters, the Consul servers run on the generic service
nodes while the admin node runs a client agent.

As explained in the <<arch-soft-consul,Software architecture>> section, Consul
_discovers_ network services on a pool of nodes. The services discovered by
Consul on Scibian HPC clusters are hosted on the generic service nodes. Each
Consul server is responsible for checking its locally running services, such as
an HTTP server for example. The state being constantly shared by all Consul
agents, every agent is actually able to tell where the services are available.
Consul notably provides a DNS interface. Given a particular virtual hostname
referring to a service, Consul can give the IP addresses of the servers
currently running this service.

Consul is not designed to operate as a full DNS server. It listens for incoming
requests on an alternative UDP port for a particular sub-domain
`virtual.<domain>`, where `<domain>` is configurable and depends on the cluster.

On the nodes, the clients are configured to connect to services in this
particular sub-domain, for example `http.virtual.<domain>` for the HTTP service.
The DNS requests sent by the clients are received by the `bind` daemon through
the virtual IP addresses of the generic service nodes, as explained in
<<arch-topics-dns_ll_ha,DNS Load-balancing and High-availability>> section. The
DNS `bind` daemon is configured to forward the requests on the virtual
sub-domain to the local Consul agent. The Consul agent answers the DNS request
with the static IP address of the generic service nodes running this service, in
random order.

In this architecture, both the DNS requests to the Consul servers and the
services (_eg._ HTTP) requests are load-balanced on all the generic service
nodes in high-availaibility mode. The same mechanism also applies to APT
proxies, Ceph RADOS gateways, and so on.

The https://github.com/edf-hpc/consult/[`Consult` utility] is installed on the
admin node to request the current status of the Consul cluster. It connects to
the REST API of the Consul client running locally and prints the status on the
standard output.
