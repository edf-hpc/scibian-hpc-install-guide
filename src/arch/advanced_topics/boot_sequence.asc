=== Boot sequence

==== Initial common steps

The servers of the cluster can boot on their hard disks or via the network,
using the PXE protocol. In normal operations, all service nodes are installed
on hard disks, and all nodes of the userspace (*compute* and *frontend* nodes)
use the network method to boot the diskless image.
A service node can use the PXE method when it is being installed.
The boot sequence between the `power on` event on the node and the boot of the
initrd is identical regardless of the system booted (installer or diskless
image).

The steps of the boot sequence are described on the diagram below:

image::src/img/boot_sequence.svg[image]

When a node boots on its network device, after a few (but generally
time-consuming) internal checks, it loads and runs the PXE ROM stored inside the
Ethernet adapter. This ROM first sends a DHCP request to get an IP address and
other network parameters. The DHCP server gives it an IP address alongside the
filename parameter. This filename is the file the PXE ROM downloads using the
TFTP protocol. This protocol, which is rather limited and unreliable is used
here because the PXE ROM commonly available in Ethernet adapters only supports
this network protocol.

The file to download depends on the type of nodes or roles. On Scibian HPC
clusters when using the Puppet-HPC software stack, the required filename for
the current node is set in Hiera in the `boot_params` hash. If not defined in
this hash, the default filename is `undionly.kpxe` which is actually the PXE
chainloaded version of iPXE for legacy BIOS systems. This filename can be
altered to support specific node settings such as virtual machine and nodes
booting in UEFI mode.

http://ipxe.org[iPXE] is open source network boot software with many advanced
features (not available in NIC PXE ROM) such scripting/menu support, HTTP and
DNS protocols support and many more. This way, it is used as a workaround to
hardware PXE ROM limitations.

The virtual machines boot like any other node, except QEMU uses iPXE as the
PXE implementation for its virtual network adapters. This means that the
virtual machines go directly to this step.

The iPXE bootloader must perform another DHCP request since the IP settings are
lost when the bootloader is loaded. The DHCP server is able to recognize this
request originates from an iPXE ROM. In this case, it sets the filename
parameter with an HTTP URL to a Python CGI script `bootmenu.py`.

The iPXE bootloader sends the GET HTTP request to this URL. In this request, it
also adds to the parameters its hostname as it was given by the DHCP server.

On the HTTP server side, the Python CGI script `bootmenu.py` dynamically
generates an iPXE boot menu for the node, with all entries available on the
cluster and the default entry set according to node settings. Please refer to
the <<arch-adv-bootmenu, iPXE Bootmenu Generator>> section for detailed
explanations about this script.

Without any action from the administrator, iPXE waits for the menu 3 seconds
timeout, then automatically selects and loads the node default boot entry set
by the CGI script.

==== Disk installation

Here is the sequence diagram of a Scibian server installation on disk, right
after the PXE boot common steps:

image::src/img/boot_sequence_diskinstallation.svg[image]

The iPXE ROM downloads the Linux kernel and the initrd archive associated with
the boot menu entry. The kernel is then run with all the parameters given
in the menu entry.

The initrd archive contains the Debian Installer program. This program starts
by sending a new DHCP request to get an IP address. Then, it downloads the
Debian installer preseed file located at the URL found in the `url ` kernel
parameter. This preseed file contains all the answers to the questions asked by
the Debian Installer program. This way, the installation process is totally
automated and does not require any interaction from the administrator.

By default on Scibian HPC clusters, this URL is directed to a Python CGI script
`preseedator.py`  which dynamically generates the preseed file for the node
given in parameter. Please refer to <<arch-adv-preseedator, Debian Installer
Preseed Generator>> section for detailed explanations about this script.

During the installation, many Debian packages are retrieved from Debian
repositories.

At the end of the installation, Debian Installer runs the commands set in the
`late_command` parameter of the preseed file. On Scibian HPC clusters, this
parameter is used to run the following steps:

* Download through HTTP the __hpc-config-apply__ script,
* Run __hpc-config-apply__ inside the chroot environment of the newly installed
system.

Detailed functionning of the __hpc-config-apply__ script is not described here,
but it involves:

* downloading and installing additional Debian packages depending on the node role,
* executing various types of software
* and writing various configuration files on the installed system.

Please refer to
https://github.com/edf-hpc/puppet-hpc/blob/master/doc/manpages/hpc-config-apply.md[`hpc-config-apply(1)` man page]
for a full documentation on how to use this script.

Finally, when the execution of the commands are over, the server
reboots.

Once the servers are installed, they are configured through IPMI with Clara
to boot on their disk devices first. Please refer to Clara documentation
for further details.

[[arch-advtpc-boot-diskless]]
==== Diskless boot

Here is the sequence diagram of the boot process for diskless nodes, right after
the PXE boot common steps:

image::src/img/boot_sequence_disklessboot.svg[image]

The iPXE bootloader downloads the Linux kernel and the initrd image defined within the
default boot menu entry and runs them with the provided parameters. Among these
parameters, there are notably:

* `fetch` whose value is an HTTP URL to a torrent file available on the HTTP
  server of the supercomputer,
* `cowsize` whose value is the size of the ramfs filesystem mounted on
  _/lib/live/mount/overlay_,
* `disk_format` if this parameter is present the device indicated is formatted
on node boot,
* `disk_raid` if this parameter is present a software raid is created with the
parameters indicated on node boot.

Within the initrd images, there are several specific scripts that come from
`live-boot`, `live-torrent` and specific Scibian Debian packages. Please refer
to the following sub-section <<Architecture,Advanced Topics, Generating diskless initrd>>
for all explanations about how these scripts have been added to the
initramfs image.

These scripts download the torrent file at the URL specified in the `fetch`
parameter, then they launch the `ctorrent` BitTorrent client. This client
extracts from the torrent file the IP address of the BitTorrent trackers and
the names of the files to download using the BitTorrent protocol.
There is actually one file to download, the SquashFS image, that the client
will download in P2P mode by gathering small chunks on several other nodes.
Then, once the file has been fully retrieved, the image is mounted after
executing some preliminary tasks like formatting the disk or setting up a raid
array if it has been indicated in the kernel options passed by the boot menu.
Then, the real init system is started and it launches all the system services.
One of these services is `hpc-config-apply.service` which runs the
__hpc-config-apply__ script.

As for the part regarding the installation with a disk, how the __hpc-config-apply__ script works is not described here.
Please refer to https://github.com/edf-hpc/puppet-hpc/blob/master/doc/manpages/hpc-config-apply.md[`hpc-config-apply(1)` man page]
for a full documentation on this topic.

Finally, the node is ready for production.
