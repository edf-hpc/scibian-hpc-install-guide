== Software architecture

=== Overview

image::src/img/arch_sw_overview.svg[image]

==== Functions

The software configuration of the cluster aims to deliver a set of functions.
Functions can rely on each other, for example, the disk installer uses the
configuration management to finish the post-install process.

The main functions provided by a Scibian HPC cluster are:

- *Configuration Management*, to distribute and apply the configuration to
  the nodes
- *Disk Installer*, to install an OS from scratch on the node disks through the
  network
- *Diskless Boot*, to boot a node with a live diskless OS through the network
- *Administrator Tools*, tools and services used by the system administrator to
  operate the cluster
- *User Tools*, tools and services used by end users

The Scibian HPC Cluster will use a set of services to deliver a particular
function. If a cluster can provide *Configuration Management* and a *Disk
Installer*, it is able to operate even if it cannot do something useful for the
users. These two core functions permit to create a self sufficient cluster
that will be used to provide other functions.

==== Services

The software services of the cluster are sorted into two broad categories:

- *Base Services*, necessary to provide core functions: install and configure a
  physical or virtual machine
- *Additional Services*, to boot a diskless (live) machine, provide all end user
  services (batch, user directory, licenses...), and system services not
  mandatory to install a machine (monitoring, metrics...)

The Base Services run on a set of physical machines that are almost identical,
those hosts are called *Service Nodes*. The services are setup to work reliably
even if some of the service nodes are down. This means that a service node can
be re-installed by other active service nodes.

The Additional Services can be installed on a set of other hosts that can be
either physical or virtual. VMs (Virtual Machines) are usually used because
those services do not need a lot of raw power and the agility provided by
virtual machines (like live host migration) are often an advantage.

If the cluster is using virtualized machines for the Additional Services, the
service nodes must also provide a consistent virtualization platform (storage
and hosts). In the reference architecture, this is provided with Ceph RBD and
Libvirtd running on service nodes.

A particular service runs on service nodes even if it is not mandatory for Disk
Installer or Config Management: the low-latency network manager (Subnet Manager
for InfiniBand, Fabric Manager for Intel Omni-Path). This exception is due to the
fact that this particular service needs raw access to the low-latency network.

In the Puppet configuration, services are usually associated with *profiles*.
For example, the puppet configuration configures the *DNS Server* service with
the profile: `profiles::dns::server`.

=== Base Services

==== Infrastructure

Infrastructure-related services provide basic network operations:

- DHCP and TFTP for PXE Boot
- DNS servers, with forwarding for external zones
- NTP servers, synchronized on external servers

These services are configured the same way and running on each service nodes.

[[arch-soft-consul]]
==== Consul

Consul is a service that permits to discover available services in the cluster.
Client will query a special DNS entry (`xxx.service.virtual`) and the DNS
server integrated with Consul will return the IP address of an available instance.

==== Ceph

Ceph provides an highly available storage system for all system needs. Ceph has
the advantage to work with internal storage on service nodes. It does not
require a storage system shared between servers (NAS or SAN).

Ceph provides:

- A Rados Block Device (RBD) that is used to store Virtual Machines disk images
- A Rados GateWay to provide storage for configuration management, Amazon S3
  compatible REST API for write operations and plain HTTP for read.
- A Ceph FS that can provide a POSIX filesystem used for Slurm Controller state
  save location

image::src/img/ceph_architecture.svg[image]

A Ceph cluster is made of four kinds of daemons. All generic service nodes run
the following daemons:

- *OSD*, Object Storage Daemons actually holding the content of the ceph
  cluster
- *RGW*, Rados GateWay (sometimes shortened radosgw) exposing an HTTP API like
  S3 to store and retrieve data in Ceph

Two other kind of service are only available on three of the generic service
nodes:

- *MON*, Monitoring nodes, this is the orchestrator of the ceph cluster. A
  quorum of two active mon nodes must be maintained for the cluster to be
  available
- *MDS*, MetaData Server, only used by CephFS (the POSIX implementation above
  ceph). At least one must always be active.

With this configuration, any server can be unavailable. As long as at least two
servers holding critical services are available, the cluster might survive
losing another non-critical server.

==== Libvirt/KVM

Service nodes are also the physical hosts for the Virtual Machines of the
cluster. Libvirt is used in combination with QEMU/KVM to configure the VMs. A
Ceph RBD pool is used to store the image of the VMs. With this configuration,
the only state on a service node is the VM definition.

[[img-arch_service_vm]]
.How the service machines, ceph and vm interact
image::src/img/arch_service_vm.svg[image]

Integration with Clara makes it easy to move VMs between nodes.

==== HTTP secret and boot

The process to boot a node needs a configuration obtained through HTTP and
computed by a CGI (in Python). This is hosted on the service nodes and served
by Apache. This is also used to serve files like the kernel, initrd and
pre-seeded configuration.

A special Virtual Host on the Apache configuration is used to serve secrets
(Hiera-Eyaml keys). This VHost is configured to only serve the files on a
specific port. This port is only accessible if the client connects from a port
below 1024 (is root), this is enforced by a Shorewall rule.

==== APT proxy

There is no full repository mirror on the cluster. APT is configured to use a
proxy that will fetch data from external repositories and cache it. This permits
to have always up-to-date packages without overloading external repositories
and without having to maintain mirror sync (internally and externally).

==== Logs

Logs from all nodes are forwarded to a Virtual IP address running on the service
nodes. The local rsyslog daemon will centralize those logs and optionally
forward the result to an external location.

==== Low-latency network manager

The Low-latency network manager (InfiniBand Subnet Manager or Intel Omni-Path
Fabric Manager) is not mandatory to achieve the feature set of Base Services
(Configuration Management and Disk Installation) but it must run on a physical
machine, so it is grouped with the Base Services to run on the service nodes.

==== NFS HA Service

A NFS HA Service can serve two purpose:

- Shared state for servicing using Posix to share their state (like
  *SlurmCtld*) when CephFS does not provided sufficient performance
- Shared storage for the users if a distributed file system like GPFS
  or Lustre is not used (only works for smaller cluster sizes)

The NFS HA Service is provided with a Keepalived setup.

=== Additional Services

==== LDAP

There is no standalone LDAP servers configured. The servers are replica from an
external directory. This means that both are configured independently and are
accessed only for read operations.

If the organization uses Kerberos, all Kerberos requests and password checks
are done directly by the external Kerberos server.

==== Bittorrent

Diskless image files are downloaded by the nodes with the BitTorrent protocol.
The cluster provides a redundant tracker service with OpenTracker and two server
machines are configured to always seed the images.

An Apache server is used to serve the torrent files for the diskless images
(HTTP Live).

==== Slurm

Slurm provides the job management service for the cluster. The controller
service (SlurmCtld) runs in an Active/Passive configuration on a pair of
servers (*batch* nodes). The state is shared between the controller nodes. This
can be achieved with a CephFS mount or with an NFS HA server. CephFS does not
permit to support a large number (thousands) of jobs yet.

The SlurmDBD service also runs on these two servers.

==== MariaDB/Galera

SlurmDBD uses a MySQL like database to store accounting information and limits.
On Scibian HPC Clusters this is provided by a MariaDB/Galera cluster which
provides an Active/Active SQL server compatible with MySQL.

This cluster is usually co-located with SlurmDBD service and Slurm Controllers
(*batch* nodes).

==== Relays

The Additional Services include a set of relay services to the outside of the
cluster for:

- Email (Postfix Relay)
- Network (NAT configured by Shorewall)
- Metrics (Carbon C Relay)

==== Monitoring

Cluster monitoring is done by Icinga2, the cluster is integrated inside an
organization Icinga infrastructure. The cluster hosts a redundant pair of
monitoring satellites that checks the nodes. The monitoring master is external
to the cluster.

=== High-Availability

All services running on the cluster should be highly available (HA). Some services
not critical for normal cluster operation can be not highly available, but this
should be avoided if possible.

The following section lists the different techniques used to achieve
high-availability of the cluster services.

==== Stateless

Stateless services are configured the same way on all servers and will give the
same answer to all requests. These services include:

- *DHCP*
- *TFTP*
- *NTP*
- *DNS*
- *LDAP Replica*
- *HTTP Secret*
- *HTTP Boot*
- *HTTP Live*
- *Ceph RadosGW*
- *APT Proxy*
- *Carbon Relay*
- *Bittorrent Tracker*
- *Bittorrent Seeder*
- *SMTP Relay*

Clients can provide a list of potential servers that will be tried in turn.
If the client do not automatically accept multiple servers, it is possible to
use the Consul service to get a DNS entry (``xxx.service.virtual``) that will
always point to an available instance of the service.

As a last resort and for services that do not need Active/Active (Load
Balancing) capabilities, it is possible to use a Virtual IP address (VIP).
*HTTP Live* and *Carbon Relay* uses this technique.

==== Native Active/Active

Some services have native internal mechanisms to share states between the
servers. Contacting any server will have the same effect on the state of the
service, or the service has an internal mechanism to get the right server.
These services behave this way:

- *Ceph Rados*
- *MariaDB/Galera*
- *Consul*

==== Native Active/Passive

Services that have only one active server at any time, but the mechanism to
select the active server is internal to the service. This means all servers are
launched in the same way and not by an external agent like Keepalived or
Pacemaker/Corosync. Services using this technique are:

- *Ceph MDS* (Posix CephFS server)
- *Slurm Controller*
- *Omni-Path Fabric Manager* or *InfiniBand Subnet Manager*

==== Controlled Active/Passive

The service can only have one active server at any one time and this failover
must be controlled by an external service. On the current configuration the
only service requiring this setup is:

- *NFS HA Server*
