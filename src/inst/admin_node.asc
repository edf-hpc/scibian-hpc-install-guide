== Admin node

Once the Service nodes are fully configured (Ceph, DNS, Consul, DHCP, TFTP,
HTTP for boot...), the cluster is able to reinstall any physical or virtual
machine with load-balancing and high-availability.

The first other node to install is the admin node, the central point of the
HPC cluster administration.

=== Base system

Add the _admin_ role by creating the file
`$ADMIN/hpc-privatedata/hieradata/$CLUSTER/roles/admin.yaml` with the
following content:

[source,yaml]
----
include::../examples/admin_role.yaml[]
----

The profiles listed after the _admin_ comment carry the software required on the
admin node. The `profiles::environment::service::packages` has a specific value
for this role in order to install the admin meta-package.

Append the node definition in the `master_network` hash, for example:

[source,yaml]
----
master_network:
  [...]
  fbadmin1:
    fqdn: "fbadmin1.%{hiera('domain')}"
    networks:
      administration:
        'DHCP_MAC': 'aa:bb:cc:dd:ee:08'
        'IP':       '10.1.0.10'
        'device':   'eno0'
        'hostname': 'fbadmin1'
      management:
        'IP':       '10.2.0.10'
        'device':   'eno1'
        'hostname': 'mgtfbadmin1'
      lowlatency:
        'IP':       '10.4.0.10'
        'device':   'ib0'
        'hostname': 'opafbadmin1'
      bmc:
        'DHCP_MAC': 'aa:bb:cc:dd:ee:09'
        'IP':       '10.2.0.110'
        'hostname': 'bmcfbadmin1'
      wan:
        'IP':       '10.2.0.10'
        'device':   'eno2'
        'hostname': 'wanfbadmin1'
----

Optionally, adjust the node boot parameters in the `boot_params` hash, for
example:

[source,yaml]
----
boot_params:
  [...]
  fbadmin1:
    os:      'scibian9'
    media:   'disk'
    console: 'ttyS0,115200n8'
----

Synchronize SSH host keys:

[source,bash]
----
puppet-hpc/scripts/sync-ssh-hostkeys.sh hpc-privatedata $CLUSTER
----

Push and apply the new configuration:

[source,bash]
----
hpc-config-push && clush -bg service hpc-config-apply -v
----

And reboot the node in PXE mode to proceed the network installation:

[source,bash]
----
export BMC=bmcfbadmin1
ipmitool -I lanplus -U ADMIN -P ADMIN -H $BMC chassis bootdev pxe
ipmitool -I lanplus -U ADMIN -P ADMIN -H $BMC power reset
----

Wait for the network installation to proceed. Once the installation is over, the
node reboot on its freshly installed system on its disks and it becomes
available through SSH. Starting from this point, all the following operations of
the installation process are realized from this admin node.

=== Administration environmnent

The administration environment must be re-created following the same
instructions given in the temporary installation node <<inst-temp-adminenv,
administration environmnet>> section.

The Clara utility is available on the admin node. Its ipmi plugin can be
configured with this small snippet added with eyaml to the cluster specific
layer of the hiera repository:

[source,yaml]
----
##### Clara #####

clara::ipmi_options:
  prefix:  'bmc'

clara::password_options:
  ASUPASSWD:   "%{hiera('cluster_decrypt_password')}"
  IMMUSER:     "%{hiera('ipmi_user')}"
  IMMPASSWORD: "%{hiera('ipmi_password')}"
----

Then add the IPMI identifiers to the admin node area layer (ex: _default_ or
_infra_) of the Hiera repository using `eyaml`:

[source,yaml]
----
ipmi_user:     DEC::PKCS7[<user>]!
ipmi_password: DEC::PKCS7[<password>]!
----

Push and apply configuration on the admin node:

[source,bash]
----
hpc-config-push && hpc-config-apply -v
----

Then, the clara ipmi plugin can be used as explained in its documentation (`man
clara-ipmi (1)`).
