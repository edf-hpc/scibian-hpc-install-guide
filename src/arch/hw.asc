== Hardware architecture

The following diagram represents at a high-level a simple typical hardware
architecture supported on Scibian HPC clusters:

[[img-hw_arch]]
.Scibian HPC cluster hardware typical architecture
image::src/img/arch_hw.svg[width=700]

=== Networks

The minimal network configuration supported on Scibian HPC clusters consists of
two physically separated networks:

* The *WAN network*, an Ethernet based network with L3 network routers which
  connect the IP networks of the HPC cluster to the organization network.

* The *backoffice network* used for basically every other internal network
  communications: deployment, services, administrator operations, etc. It must
  be an Ethernet network with dedicated (level 2 or more) switches.

For performance reasons with distributed computing, HPC clusters generally have
a third *low-latency network*. It is used for both I/O to the main storage
system and distributed computing communications (typically MPI messages) between
compute nodes. The hardware technologies of this network may vary upon
performance requirements but it generally involves high bandwidth (10+GB/s) and
low latency technologies such as InfiniBand, Omni-Path or 10GB Ethernet. In the
absence of dedicated low-latency network, the *backoffice network* is also used
for central storage system I/O and distributed computing communications.

It is recommended to split the *backoffice network* with four VLAN dedicated to
the following groups of network interfaces:

* The system interfaces of the infrastructure cluster nodes,
* The system interfaces of the userspace cluster nodes,
* The management interfaces of the infrastructure cluster nodes (BMC
  footnote:[Baseboard Management Card]) and hardware equipments (switches,
  storage controllers, CMC footnote:[Chassis Management Card], etc),
* The management interfaces of the userspace cluster nodes.

In this setup, it is recommended to route IP subnetworks between the VLAN with
L3 switche(s) on the *backoffice network*.

This setup has significant advantages both in terms of reliability and security:

* It significantly reduces the size of Ethernet broadcast domains which notably
  increases DHCP reliability and drops Ethernet switches load.
* It makes easier to restrict access to the infrastructure cluster and hardware
  equipments, notably in case of evil user attack.
* It gives the possibilty to fully adopt the *areas* feature of Puppet-HPC.

NOTE: For more details about the *areas* feature, please refer to _Puppet-HPC
Reference Documentation_ (chapter _Software Architecture_, section
_Cluster Definition_).

=== Infrastructure cluster

The infrastructure cluster is composed by two types of nodes: the *admin node*
and the *generic service nodes*.

The *admin node* is the access node for administrators and the central point of
administrative operations. All common administrative actions are performed on
this node. It does not run any intensive workloads, just simple short-lived
programs and it does not need to be very powerful. It does not store sensible data
nor run critical services, so it does not need to be very reliable either. Example
of hardware specifications:

[cols="1,3a"]
|===

|CPU
|1 x 4 cores

|RAM
|8GB ECC

|Network
|* 1 x 1GB bonding on backoffice network
* 1 x 1GB bonding on WAN network
* 1 link on low-latency network

|Storage
|2 x 300GB RAID1 SATA hard disk

|PSU
|Non-redundant

|===

The *generic service nodes* run all critical infrastructure services (within
service virtual machines) and manage all production administrative data.
Scibian HPC requires a pool from 3 (minimum) to 5 (recommended) generic service
nodes. The pool works in active cluster mode, the load is balanced with
automatic fail-over. All generic service nodes of a cluster must be fairly
identical for efficient load-balancing.

The generic service nodes manage the production data into a distributed
object-storage system. It is highly recommended that the nodes have a dedicated
block storage device for this purpose. The workload is mostly proportional to
the number of compute nodes but the generic service nodes must be quite powerful
to comfortably handle load peaks happening during some operations (**ex:** full
cluster reboot). Also, since services are run into virtual machines, a
fairly large amount of RAM is required. Services can generate a lot of
traffic on the backoffice network, it is relevant to provide a network adapter
with high bandwidth. Even though high-availability is ensured at the software
level with automatic fail-over between generic service nodes, it is nevertheless
recommended to get hardware redundancy on most devices of the generic service
nodes to avoid always risky and hazardous service migrations as much as
possible. Example of hardware specifications:

[cols="1,3a"]
|===

|CPU
|2 x 16 cores

|RAM
|64GB ECC

|Network
|* 2 x 10GB bonding on backoffice network
* 2 x 1GB bonding on WAN network
* 1 link on low-latency network

|Storage
|* 2 x 300GB RAID1 SATA hard disk for host
* 2 x 1TB SSD SAS or NVMe PCIe for object-storage system

|PSU
|Redundant

|===

All physical nodes must be connected to all three physical networks. There are
virtual bridges on the host of the generic service nodes connected to the WAN
and backoffice networks. The service virtual machines have connections to the
virtual bridges upon their hosted service requirements.

=== User-space cluster

The user-space cluster is composed of *frontend nodes* and *compute nodes*.

The nodes of the user-space cluster are deployed with a diskless live system
stored in RAM. It implies that, technically speaking, the nodes do not
necessarily need to have local block storage devices.

The *frontend nodes* are the access hosts for users so they must be connected to
all three physical networks. It is possible to have multiple frontend nodes in
active cluster mode for load-balancing and automatic fail-over. The exact
hardware specifications of the frontend nodes mostly depend on user needs and
expectations. Users may need to transfer large amount of data to the cluster, it
is therefore recommended to provide high-bandwidth network adapters for the WAN
network. These nodes can also be designed to compile computational codes and
in this case, they must be powerful in terms of CPU, RAM and local storage I/O.

The *compute nodes* run the jobs so they must provide high performances. Their
exact hardware specifications totally depend on user needs. They must be
connected to both the backoffice and the low-latency networks.

=== Storage system

The storage system is designed to host user data. It provides one or
several shared POSIX filesystems. The evolved storage technologies depend
on user needs ranging from a simple NFS NAS to a complex distributed
filesystem such as Lustre or GPFS with many SAN and I/O servers.
