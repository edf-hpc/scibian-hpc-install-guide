== Password/keys changes

=== Root password

The hashed root password is stored in the variable
`profiles::cluster::root_password_hash` in yaml files. The value must be
encrypted using eyaml. It can be simply changed using the `eyaml` command.

----
# eyaml edit cluster.yaml
...
profiles::cluster::root_password_hash: DEC::PKCS7[hashed_password]!
...
----

Once changed, the new configuration must be applied on all the machines of the
cluster.

=== Root SSH key

The root SSH keys are stored in the internal repository. The privates keys must
be encrypted. The SSH public rsa key is also in the variable
`openssh::server::root_public_key`. It is necessary to change the files and the
value of the variable at the same time. To avoid connections problems, it is
necessary to follow these steps in this order:

1. Change the keys files and the variable `openssh::server::root_public_key` in
the internal repository
2. Apply the configuration on all the machines exept the *admin* one
3. Apply the new configuration on the *admin* server.

NOTE: In case of desynchronization between the keys on the *admin* node and
those on the others nodes, it is always possible to use the root password to
connect.

=== SSH host keys

==== SSH host keys management script

On Scibian HPC clusters, SSH host keys are managed by Puppet-HPC utility script
https://github.com/edf-hpc/puppet-hpc/blob/master/scripts/sync-ssh-hostkeys.sh[`scripts/sync-ssh-hostkeys.sh`].

This script extracts the list of cluster nodes in `master_network` hash in
`hpc-privatedata/hieradata/$CLUSTER/network.yaml`  having a role declared in
`hpc-privatedata/hieradata/$CLUSTER/roles/` directory. For all of these cluster
nodes, it generates the SSH host keys pair and encodes the private key file
using the `cluster_master_password` of the main area. The generated files are
located in `hpc-privatedata/files/$CLUSTER/$AREA/hostkeys/` directory,
depending on the area of the targeted node. These keys are then deployed on
nodes by Puppet-HPC.

The script only generates the *missing* SSH host keys. If a key pair already
exists in the directory, it is skipped.

After the script ensured all the SSH host keys are properly generated, it
generates a SSH known hosts file with all cluster nodes public keys, located in
`hpc-privatedata/files/$CLUSTER/cluster/ssh/known_hosts`. This file is then
deployed on all cluster nodes as the system-wide
`/etc/ssh/ssh_known_hosts` file. This way, all users (including root) do not
have to populate and maintain their own `~/.ssh/known_hosts` file.

Optionally, the SSH host keys management script is also able to scan the SSH
host keys of a list of other hosts (ex: external nodes or equipments) and append
the discovered keys to the generated system-wide known hosts file. This is
especially useful when SSH strict host keys checking is enabled

The corresponding procedures are fully explained in the following sub-sections.

==== Renew cluster nodes SSH hostkeys

To avoid connections problems, it is necessary to follow these steps in this
order:

. Remove the keys to renew in `hpc-privatedata/files/$CLUSTER/$AREA/hostkeys/`
directory, depending on the area of the targeted node.

. Run Puppet-HPC SSH host keys management scripts:
+
[source,bash]
----
cd $ADMIN && puppet-hpc/scripts/sync-ssh-hostkeys.sh hpc-privatedata $CLUSTER
----
+
It should mention the new keys are generated.

. Push and apply the configuration environment on all nodes:
+
[source,bash]
----
hpc-config-push && clush -bg all hpc-config-apply -v [--tags openssh]
----
+
You can optionally restrict to `openssh` tag for more precise configuration
updates on nodes.

. Eventually proceed to some cleanup in `/root/.ssh/known_hosts` on all
nodes. In a perfect world, this would not even exist and can be safely removed.

==== External hosts SSH host keys

The Puppet-HPC SSH host keys management script is able to scan any arbitrary
SSH hosts keys to fill the cluster system-wide known hosts file. The list of
hosts must be defined in file
`hpc-privatedata/files/$CLUSTER/cluster/ssh/external_hosts`.

The format is one host per line. All hosts must be declared with their FQDN.

It is possible to add comments in file with lines starting with `#`.

In order to add or remove external hosts SSH host keys from cluster system-wide
known hosts files, follow these steps:

. Edit `hpc-privatedata/files/$CLUSTER/cluster/ssh/external_hosts`
. Run Puppet-HPC SSH host keys management scripts:
+
[source,bash]
----
cd $ADMIN && puppet-hpc/scripts/sync-ssh-hostkeys.sh hpc-privatedata $CLUSTER
----
+
It should update `hpc-privatedata/files/$CLUSTER/cluster/ssh/known_hosts` file.

. Push and apply the configuration environment on all nodes:
+
[source,bash]
----
hpc-config-push && clush -bg all hpc-config-apply -v [--tags openssh]
----
+
You can optionally restrict to `openssh` tag for more precise configuration
updates on nodes.

. Eventually proceed to some cleanup in `/root/.ssh/known_hosts` on all
nodes. In a perfect world, this would not even exist and can be safely removed.

=== Eyaml keys

Replacing the eyaml PKCS7 key pair consist in reality of two actions:

1. Generate a new pair of keys (`eyaml createkeys`)
2. Replace all the values encoded with the old pair with ones encoded with the
new pair of keys.

NOTE: As these operations implies decoding files and re-encoding them with
another key pair, it is not possible to perform other administrative
operations (like applying the configuration on nodes) on the cluster at the
same time. The changing keys operation must be fully completed before resuming
"normal" administrative operations.

These steps must be followed in order to safely change the eyaml keys:

Save the old keys:

----
# cp /etc/puppet/secure/keys/private_key.pkcs7.pem \
     /etc/puppet/secure/keys/private_key.pkcs7.pem.old
# cp /etc/puppet/secure/keys/public_key.pkcs7.pem \
     /etc/puppet/secure/keys/public_key.pkcs7.pem.old
----

Copy the new keys in __/etc/puppet/secure/keys/__.

Decrypt all the yaml files encoded using the old keys:

----
# eyaml decrypt \
  --pkcs7-private-key /etc/puppet/secure/keys/private_key.pkcs7.pem.old \
  --pkcs7-public-key /etc/puppet/secure/keys/public_key.pkcs7.pem.old \
  -e hieradata/<cluster>/cluster.yaml \
  > hieradata/<cluster>/cluster.decrypt.yaml
----

The `decrypt.yaml` contains all the secret in plain text. It should be removed
as soon as possible.

Encrypt the files with the new keys:

----
# eyaml encrypt -e hieradata/<cluster>/cluster.decrypt.yaml \
  > hieradata/<cluster>/cluster.yaml
# rm hieradata/<cluster>/cluster.decrypt.yaml
----

Remove the old saved keys from the *admin* node:

----
# rm /etc/puppet/secure/keys/private_key.pkcs7.pem.old \
     /etc/puppet/secure/keys/public_key.pkcs7.pem.old
----

Create a tarball, encode it with `clara enc` and add it to the __files__
directory of the internal repository:

----
# tar cJf /tmp/keys.tar.xz -C /etc/puppet/secure keys
# clara enc encode /tmp/keys.tar.xz
# mv /tmp/keys.tar.xz.enc <internal repository>/files/<cluster>/eyaml
----

Where:

* <internal repository> is the directory that contains the clone of the internal
repository.
* <cluster> is the name of the cluster.

At this stage, the keys are now stored encrypted in the internal repository and
are available locally in the standard eyaml paths.

In the default Scibian-HPC configuration, the PKCS7 keys propagation service
runs on all the generic service nodes. First, the encoded tarball must be
manually copied on the nodes:

----
# scp <internal repository>/files/<cluster>/eyaml/keys.tar.xz <generic server X>:/tmp
----

Where <generic server X> is the hostname of the generic service node.

Then apply the configuration using the new keys:

----
# hpc-config-apply -vv --keys-source=/tmp
----

This will copy the eyaml PKCS7 key pair in the right directory to be serviced
by the propagation service to all others nodes when applying the puppet
configuration.
These last two operations must be executed on all the generic service nodes.

Don't forget to remove the keys from the `/tmp` directory on the admin node and
on all the service nodes.

----
# rm /tmp/keys.tar.xz
# clush -w @service rm /tmp/keys.tar.xz
----

=== Internal repository encoding key

NOTE: As these operations implies decrypting files and re-encrypting them with
another key, it is not possible to perform other administrative operations
(like applying the configuration on nodes) on the cluster at the same time.
The changing key operation must be fully completed before resuming "normal"
administrative operations.

Replacing the AES key used to encode files in the internal repository consist in
several steps.

Generate a new AES key:

----
# openssl rand -base64 32
----

For each encoded file in the internal repository, it is necessary to decode it
with the old key and re-encode it with the new one.

----
# clara enc decode <internal repository>/files/<cluster>/<filename>.enc
# openssl aes-256-cbc \
          -in <internal repository>/files/<cluster>/<filename> \
          -out <filename>.enc -k <AES KEY>
# rm <internal repository>/files/<cluster>/<filename>
----

Where:

* <internal repository> is the directory that contains the clone of the internal
repository
* <cluster> is the name of the cluster
* <filename> is the path of the file to encode
* <AES KEY> is the random 256 bits key.

Using `clara` for both operations, decode and encode, is not possible as it
support only one AES key.

This re-encryption step can be automated with the `reencode-file.sh` script in
the `puppet-hpc` scripts dir:

----
# cd <internal repository>/files/<cluster>
# find -name "*.enc" \
  -exec <puppet-hpc path>/scripts/reencode-file.sh\
    /tmp/oldkey /tmp/newkey '{}' ';'
----

The files `/tmp/oldkey` and `/tmp/newkey` are files with just the old and new
AES key respectively. This script does not depend on `clara` but basically
performs the same steps as above.

The AES key must be placed in __cluster_decrypt_password__ in the cluster layer
of the Hiera repository:

----
# eyaml edit hieradata/<cluster>/cluster.eyaml
----

Replace the key:

----
cluster_decrypt_password: DEC::PKCS7[<AES KEY>]!
----

Apply the new configuration on the *admin* node, to update `clara`
configuration:

----
# hpc-config-apply
----

=== Replication account password



The steps to change these credentials are described here:

1. Decode the configuration ldif file:

    # clara enc edit <internal repository>/files/<cluster>/<filename>.enc

2. The field to change is `olcSyncrepl:`, it contains all the necessary
informations to connect to the master LDAP server (login, password, URI, etc ..)

3. Apply the new configuration on the *proxy* nodes.

4. Follow the LDAP bootstrap procedure as described in <<bootstrap-ldap,
LDAP bootstrap>> on each *proxy* node. It is recommended to wait until the first
ldap replicate is complete before attempting to update the second, to not disrupt
authentication across the cluster.

NOTE: It is possible to change others values with this procedure, for example
the root LDAP password.

=== Monitoring certificates

The certificates used for monitoring are stored, encrypted, in the internal
repository in __<internal repository>/files/<cluster>/icinga2/certs/__. Each
host has a certificate and a key.
The steps to follow to change them are:

1. Change the key and certificate files in the internal repository
2. Apply the configuration on the concerned node
3. Update the certificate on the Icinga2 server

=== Munge key

NOTE: Scheduling service and jobs must be stopped to change the munge key.

WARNING: This will kill running jobs.

1. Stop the `slurmd` and `slurmctld` daemons.


2. Stop the munge daemon on all nodes.
3. Encrypt the new key with `Clara` and place it in
__<internal repository>/files/<cluster>/munge/munge.key.enc__
4. Apply the new configuration on all nodes.
5. restart the daemons.

=== Repo keyring

NOTE: The packages must be saved in another place.

The cluster must use a private cluster keyring. This keyring is used to sign the
local packages repository.

It is stored in the internal repository:
__<internal repository>/files/<cluster>/repo/__

Here are the steps to follow to change it:

1. Generates a new keyring:

    # LANG=C gpg --no-default-keyring \
    --keyring <internal repository>/files/<cluster>/repo/cluster_keyring.gpg \
    --secret-keyring <internal repository>/files/<cluster>/repo/cluster_keyring.secret.gpg \
    --gen-key

2. Encode the secret file with `clara encode`.
3. Apply the configuration on the *admin* node.
4. Delete the folder containing the local repository.
5. Re-create the repository with `clara`:

   # clara repo key
   # clara repo init scibian9-hpc

6. Add the previously saved packages with `clara`:

   # clara repo add scibian9-hpc mypackage_1-2.dsc
   # ...

=== MariaDB users

Generate passwords conform with your organization policy and edit the following
parameters with `eyaml` in the hiera repository:

* `slurmdbd_slurm_db_password`
* `slurmdbd_slurmro_db_password`

These parameters correspond to the passwords of the MariaDB having respectively
R/W and R/O grants on the SlurmDBD database.

Once modified, push and apply the configuration with the following commands:

----
# hpc-config-push && \
  clush --fanout=1 -bg batch hpc-config-apply -v
----

The `hpc-config-apply` command will perform the following steps, on each batch
node:

* Update the passwords in the configuration file of the Slurm `mysql-setup`
  utility.
* Update the passwords in the MariaDB database
* Update SlurmDBD configuration (if R/W password changed)
* Restart SlurmDBD (if R/W password changed)

The `--fanout=1` parameter of the `clush` command makes sure the configuration
is not applied simultaneously on both batch nodes. This could cause the SlurmDBD
daemon to be restarted at the same time and make this service unavailable for a
short period of time.
