= Installation procedures

////
TODO: fill this chapter to document all the steps to follow in order to install
a Scibian HPC cluster.
////

== Internal repository

////
TODO: document the steps to initialize the internal hieradata repository
////

== Cluster definition

////
TODO: document how-to write cluster definition in hiera, with reference to
[Puppet-HPC reference documentation]
////

This section guides you through the steps to define your cluster in Hiera. 
More detailed information can be found in 
https://github.com/edf-hpc/puppet-hpc/blob/master/doc/src/arch.asc#cluster-definition[the 
Puppet-HPC reference documentation, Software Architecture, Cluster Definition].

=== Global description of the cluster

First, the main shared parameters defining your cluster must be specified. This
is usually done in a file named `cluster.yaml`.

```
## Global description of the cluster
cluster_name:      'generic'                        # Name of the cluster
cluster_prefix:    'gen'                            # Prefix used in node name  
private_files_dir: "/admin/production/latest/files" # Where configuration files are stored
domain:            'generic.hpc.domain.org'         # Domain name used accross 
                                                    # all the machines
user_groups:                                        # Array of user groups 
  - "cl-%{hiera('cluster_name')}-users-dep1"        # allowed to access to the
  - "cl-%{hiera('cluster_name')}-users-dep2"        # cluster
  - "cl-%{hiera('cluster_name')}-users-dep2"
cluster_decrypt_password: 
                    'my_encrypted_password'         # Password encrypted with e-yaml
```                    

=== Cluster networks 

Once these parameters have been defined, the different networks used in the
cluster must be described. The 2 files containing the network details are
`cluster.yaml` and `network.yaml`.

==== In cluster.yaml

===== Topology

The network topology is usually specified in the file `cluster.yaml`. 
Typically, a cluster will have :

* some nodes connected to the enterprise WAN,
* a local administration network,
* a local low latency network efficient for calculations,
* a local management network including a subnet dedicated to Board Management Cards (bmc).

===== Bonding

If bonding is used between some network interfaces on some nodes of the 
cluster, it must be defined in `cluster.yaml`. The `network::bonding_options` 
should be used to specify the options for each bond.

===== Bridges

In the same way, it is also possible to set up bridge interfaces. 
The specification should be done by defining the `network::bridge_options`.

===== Example

A full example of network definition in `cluster.yaml` is detailed in
https://github.com/edf-hpc/puppet-hpc/blob/master/doc/src/arch.asc#network-definitions[the section Network Definitions of the Puppet-HPC Reference].

==== In network.yaml

The file `network.yaml` includes a hash called `master_network` that lists a 
description of network settings for all the nodes in the cluster. A detailed 
example is available in
https://github.com/edf-hpc/puppet-hpc/blob/master/doc/src/arch.asc#node-definitions[the section Node Definitions of the Puppet-HPC Reference].

== Generate keys

////
TODO: document the steps to generate the keys, certificates, and so on.
////
 
== Install admin node

////
TODO: document the steps to boot the admin node with rikenter and
install/configure minimal installation system in live.
////

== Install generic servers

////
TODO: document how-to install and configure generic  nodes.
////

== Configure Ceph

=== Architecture

image::src/img/ceph_architecture.svg[image]

A ceph cluster is created on the service nodes. A ceph cluster is made of four
kinds of daemons. All service nodes will have:

- *OSD*, Object Storage Daemons actually holding the content of the ceph
  cluster
- *RGW*, Rados GateWay (sometimes shortened radosgw) exposing an HTTP API like
  S3 to store and retrieve data in Ceph

Two other kind of service are only available on three of the service nodes:

- *MON*, Monitoring nodes, this is the orchestrators of the ceph cluster. A
  quorum of two active mon nodes must be maintained for the cluster to be
  available
- *MDS*, MetaData Server, only used by CephFS (the POSIX implementation above
  ceph). At least one must always be active.

With this configuration, any server can be unavailable. As long as at least two
servers holding critical services are availble, the cluster might survive
losing another non-critical server. 

=== Deployment

Deployment is based on a tool called `ceph-deploy`. This tool performs the
steps on a node to setup a ceph component. `ceph-deploy` is used to setup a
node, once the cluster is running, the configuration is reported in the Puppet
configuration in case it is re-deployed.

The reference configuration uses one disk (or hardware RAID LUN) to hold the
system (`/dev/sda`) and another to hold the Ceph OSD data and journal
(`/dev/sdb`).

Three or five nodes must be chosed to setup the *MON* and *MDS* services, the
remaining nodes are used only as *OSD* and *RadosGW* nodes.

To work, `ceph-deploy` must run on a node with a passwordless SSH connection to
the service nodes, directly to root or to a user with passwordless sudo.

The `ceph-deploy` tool will generate authentication keys for ceph. Once the
cluster is running, theses keys should be manually collected and encrypted with
`eyaml` to be included in the *hiera* configuration.

#THTODO Report ceph.txt steps

In case of generic service node reinstallation after the initial configuration,
bootstrap steps may be necessary:

- *MDS* and *RadosGW*, those services have no state outside of Rados, so no
  additional bootstrap is necessary
- *Mon* Always necessary to bootstrap
- *OSD* Must be bootstraped if the OSD volume (`/dev/sdb`) is lost.

=== Rados GateWay

#THTODO step to create S3 config compatible with consul

== Re-install admin node

////
TODO: document how-to install admin node with generic service nodes.
////

== Install virtual machines

////
TODO: document how-to install and configure virtual machines with references to
[bootstraping guides] for service.
////

== Build diskless image

Diskless nodes use squashfs images downloaded at boot. A simple way to
generate these images is to use https://github.com/edf-hpc/clara[`clara`].
Once configured, `clara image create` can be used to build Debian image(s).

Clara can install a selected list of packages into the image, manage
the repositories used and add specific files and directories.

Example:
-------------------------------------------------------------------------------
# clara image create scibian8
-------------------------------------------------------------------------------

Clara can also generate an `initrd` (initial ramdisk) to use with the image.
It can download the image either by http or bitorrent protocol.

Example:
-------------------------------------------------------------------------------
# clara image initrd scibian8
-------------------------------------------------------------------------------

Initrd must be regenerated each time the kernel image is modified.

See the http://edf-hpc.github.io/clara/[clara documentation]  for a full
list of options and possibilities.

Every time the image and the initrd are regenerated, they must be made
available to the deployment system.

== Boot nodes

To remotely start the nodes, you can use https://github.com/edf-hpc/clara[`clara`].
The ipmi plugin from clara is used for powering on/off selected nodes,
managing the boot node mode (if you want to install OS on disk or load it in RAM)
and accessing to the remote console.

See the http://edf-hpc.github.io/clara/[clara documentation] for a full
list of options and possibilities.

The commands from the following example, show how to reboot a node with
the boot menu, choose the operating system and boot mode between "diskless"
or "on disk".

Example:
-------------------------------------------------------------------------------
# clara ipmi pxe <node>
# clara ipmi reboot <node>
# clara ipmi connect <node>
-------------------------------------------------------------------------------

After the last command, when the login prompt is shown, you can connect on node
as root and monitor the progress of the puppet configuration by running
the following command:

-------------------------------------------------------------------------------
# journalctl -f -u hpc-config-apply
-------------------------------------------------------------------------------

When the network is up in the node you can also disconnect from the console
(press on "!" and "." keys as explained on the clara documentation) and
connect to the node via ssh.
