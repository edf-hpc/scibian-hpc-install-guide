[[inst-slurm]]
== Slurm

Slurm workload manager is distributed among the HPC cluster nodes with multiple
daemons and clients software. On Scibian HPC clusters, the server part of Slurm,
_ie._ the controller and the accounting services, run in high-availability mode
on the _batch_ nodes. These components are managed by the `jobsched::server`.
The _batch_ nodes also need the `db::server` and the `ceph::client` that
respectively setup the MariaDB galera RDBMSfootnote:[Relational Database
Management System] and CephFS filesystem client.

Slurm communications between nodes are secured using Munge which is based on a
secret shared key. Generate this munge key with the following command:

----
# mkdir -p $ADMIN/hpc-privatedata/files/$CLUSTER/munge
# dd if=/dev/urandom bs=1 count=1024 > $ADMIN/hpc-privatedata/files/$CLUSTER/munge/munge.key
----

Encrypt the key using Clara:

----
# clara enc encode $ADMIN/hpc-privatedata/files/$CLUSTER/munge/munge.key
----

Remove the unencrypted key:

----
# rm $ADMIN/hpc-privatedata/files/$CLUSTER/munge/munge.key
----

Setup the nodes and partitions managed by Slurm in the
`slurm::partitions_options` hash in the cluster specific layer of the Hiera
repository. For example:

[source,yaml]
----
slurm::partitions_options:
  - 'NodeName=fbcn[01-04] Sockets=2 CoresPerSocket=14 RealMemory=64000 State=UNKNOWN'
  - 'NodeName=fbgn01 Sockets=2 CoresPerSocket=4 RealMemory=64000 Gres=gpu:k80:2 State=UNKNOWN'
  - 'PartitionName=cn Nodes=fbcn[01-04] Default=YES MaxTime=INFINITE State=UP'
  - 'PartitionName=gn Nodes=fbgn01 MaxTime=INFINITE State=UP'
  - 'PartitionName=all Nodes=fbcn[01-04],fbgn01 MaxTime=INFINITE State=UP'
----

Please refer to https://slurm.schedmd.com/[Slurm documentation] for more details
about these settings.

In the same, setup the LDAP/SlurmDBD users synchronization utility, for example:

[source,yaml]
----
profiles::jobsched::server::sync_options:
  main:
    cluster: "%{hiera('cluster_name')}"
    org:     "%{hiera('org')}"
    policy:  'global_account'
  global_account:
    name:    'users'
    desc:    'Main users account'
----

Please refer to the
https://github.com/edf-hpc/slurm-llnl-misc-plugins/blob/master/sync-accounts/sync-accounts.conf[example
configuration file] for more details.


Still in the cluster specific layer of the Hiera repository, setup the CephFS
client mount with the following excerpt:

[source,yaml]
----
profiles::jobsched::server::ceph::keys:
  client:
    key: "%{hiera('ceph_client_admin_key')}"

profiles::jobsched::server::ceph::mounts:
  slurmctld:
    servers: # list of Ceph MON servers
      - fbservice2
      - fbservice3
      - fbservice4
    device:     '/slurmctld'
    mountpoint: "%{hiera('slurm_state_save_loc')}"
    user:       'admin'
    key:        'client'
    mode:       'kernel'
----

Eventually, it is possible to tune Slurm, GRES, SlurmDBD, job submit LUA script
with the following parameters:

[source,yaml]
----
profiles::jobsched::slurm_config_options:
  PrivateData:              'jobs,reservations,usage'
  AccountingStorageEnforce: 'associations,limits,qos'
  GresTypes:                'gpu'
  SlurmCtldDebug:           'verbose'
  PriorityFlags:            'FAIR_TREE'

slurm::gres_options:
  - 'NodeName=fbgn01 Name=gpu Type=k80 File=/dev/nvidia0'

profiles::jobsched::server::slurmdbd_config_options:
  PrivateData: 'accounts,jobs,reservations,usage,users'

slurm::ctld::submit_lua_options:
  CORES_PER_NODE:  '28'
----

Once the configuration is set in the Hiera repository, push and apply the
configuration on the _admin_ and _batch_ nodes:

----
# hpc-config-push && clush -bg admin,batch hpc-config-apply -v
----

Some software components need to be manually bootstrapped on the _batch_ nodes
before being started:

* <<bootstrap-mariadb,MariaDB>> database
* <<bootstrap-slurmdbd,SlurmDBD>> service
* <<bootstrap-ceph-fs,CephFS>> filesystem

Please refer to the <<bootstrap,Bootstrap procedure chapter>> of this document
for all details.

Finally, re-apply the configuration on the _batch_ nodes to launch all Slurm
server-side software services with:

----
# clush -bg batch hpc-config-apply -v
----

Check Slurm is available by running the `sinfo` command on the admin node. If
the command report the nodes and partitions state without error, Slurm is
properly running.
