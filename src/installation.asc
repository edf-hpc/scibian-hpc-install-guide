= Installation procedures

////
TODO: fill this chapter to document all the steps to follow in order to install
a Scibian HPC cluster.
////

== Set up Configuration Management 

The first step in installing a Scibian HPC Cluster is to have an operationnal
Configuration Management functionnality. In Scibian HPC Clusters, this function
is based on the Puppet-HPC software stack. 

=== Internal repository

////
TODO: document the steps to initialize the internal hieradata repository
////

In addition to 
https://github.com/edf-hpc/puppet-hpc[the GitHub repository of Puppet-HPC], 
an internal Git repository containing all specific data is necessary. If this 
internal repository does not exist yet, it should be created on an internal 
forge or Git server. It must be accessible from the admin node of the cluster 
being installed.

Its structure and content are detailed in 
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the 
Puppet-HPC reference documentation], 
in Software Architecture, Internal Repository.

For example, if you are installing a cluster `cluster1` and creating its
internal repository, its layout should look like this.

```
[git_server]$tree internal_rep
internal_rep/
|-- files
|   `-- cluster1
|-- hieradata
|   `-- cluster1
`-- puppet-config
    `-- cluster1
```

=== Configure Puppet and Hiera

////
TODO: document how-to write puppet.conf, hiera.yaml and which functionnalities are necessary (eyaml) with reference to [Puppet-HPC reference documentation]
////

The `puppet-config/cluster1` subdirectory of the internal Git repository should
 contain all the files necessary to set up Puppet and Hiera on each node of 
`cluster1`.

==== puppet.conf

The Puppet configuration file, `puppet.conf` common to all the nodes should be
stored in `puppet-config/cluster1`. 
For example, if Puppet < 4 is used in your environment, it is required to add 
the setting `stringify_facts=false` to `puppet.conf` so that you can use 
Puppet-HPC, as specified in 
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC reference documentation],
in Development Guidelines, Language Settings.

Here is below the typical content of the `puppet.conf` file to add to
`puppet-config/cluster1` when the version of Puppet is < 4.

```
[main]
logdir=/var/log/puppet
vardir=/var/lib/puppet
ssldir=/var/lib/puppet/ssl
rundir=/var/run/puppet
environmentpath = $confdir/environments
prerun_command=/etc/puppet/etckeeper-commit-pre
postrun_command=/etc/puppet/etckeeper-commit-post
stringify_facts=false
hiera_config=$confdir/hiera.yaml

[master]
# These are needed when the puppetmaster is run by passenger
# # and can safely be removed if webrick is used.
# ssl_client_header = SSL_CLIENT_S_DN
# ssl_client_verify_header = SSL_CLIENT_VERIFY
```

==== hiera.yaml

The Hiera configuration file, `hiera.yaml` should also be common to all the 
cluster nodes. It is usually stored in `puppet-config/cluster1`.

It is used to indicate which backend should be used for Hiera and the hierarchy
 to take into account. If Hiera-Eyaml is used to encrypt some data, it should 
also be specified in this file.

Puppet-HPC uses the `yaml` backend of Hiera and encrypts passwords with
Hiera-Eyaml. A typical `hiera.yaml` configuration file for Puppet-HPC would
look like the following.

```
:backends:
  - eyaml
:eyaml:
  :datadir:           /etc/puppet/environments/%{environment}/hieradata
  :pkcs7_private_key: /etc/puppet/secure/keys/private_key.pkcs7.pem
  :pkcs7_public_key:  /etc/puppet/secure/keys/public_key.pkcs7.pem
  :extension:         'yaml'
:hierarchy:
  - private/%{cluster_name}/roles/%{puppet_role}
  - private/%{cluster_name}/cluster
  - private/%{cluster_name}/network
  - private/edf_hpc
  - generic/common
  - generic/%{osfamily}/common
```

More information about how Hiera-Eyaml is used to deal with sensitive data
in Puppet-HPC can be found in
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC reference documentation],
in Software Architecture, Sensitive Data Encryption.

==== hpc-config-facts.yaml

This file can be used to define additional facts common to all the nodes of the
cluster, for example the name of the cluster. This file should also be stored
in `puppet-config/cluster1` of the internal Git repository.

Here is an example of `hpc-config-facts.yaml`

```
---
cluster_name: 'cluster1'
other_cluster_fact: 'toto'
```

It is copied by `hpc-config-apply` locally on each node under 
`/var/lib/puppet/facts.d` so that its content is published as facts.


=== Cluster definition

////
TODO: document how-to write cluster definition in hiera, with reference to
[Puppet-HPC reference documentation]
////

Still in the case of a cluster named `cluster1`, the `hieradata/cluster1` 
subdirectory of the internal Git repository should contain the 
full description of the cluster being installed, also referred to as cluster 
definition.

This section guides you through the steps to define your cluster in Hiera. 
More detailed information can be found in 
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the 
Puppet-HPC reference documentation], 
in Software Architecture, Cluster Definition.

==== Global description of the cluster

First, the main shared parameters defining your cluster must be specified. This
is usually done in a file named `cluster.yaml`.

```
## Global description of the cluster
cluster_name:      'cluster1'                       # Name of the cluster
cluster_prefix:    'cl'                             # Prefix used in node names  
private_files_dir: "/admin/production/latest/files" # Where configuration files are stored
domain:            'cluster1.hpc.domain.org'        # Domain name used accross all the 
                                                    # machines
user_groups:                                        # Array of user groups allowed to access
  - "cl-%{hiera('cluster_name')}-users-dep1"        # to the cluster
  - "cl-%{hiera('cluster_name')}-users-dep2"       
  - "cl-%{hiera('cluster_name')}-users-dep2"
cluster_decrypt_password: 
                    'my_encrypted_password'         # Password encrypted with Hiera-Eyaml
```                    

==== Cluster networks 

Once these parameters have been defined, the different networks used in the
cluster must be described. The 2 files containing the network details are
`cluster.yaml` and `network.yaml`.

===== In cluster.yaml

====== Topology

The network topology is usually specified in the file `cluster.yaml`. 
Typically, a cluster will have :

* some nodes connected to the enterprise WAN,
* a local administration network,
* a local low latency network efficient for calculations,
* a local management network including a subnet dedicated to Board Management Cards (bmc).

====== Bonding

If bonding is used between some network interfaces on some nodes of the 
cluster, it must be defined in `cluster.yaml`. The `network::bonding_options` 
should be used to specify the options for each bond.

====== Bridges

In the same way, it is also possible to set up bridge interfaces. 
The specification should be done by defining the `network::bridge_options`.

====== Example

A full example of network definition in `cluster.yaml` is detailed in
the section Network Definitions of 
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the 
Puppet-HPC Reference].

===== In network.yaml

The file `network.yaml` includes a hash called `master_network` that lists a 
description of network settings for all the nodes in the cluster. A detailed 
example is available in the section Node Definitions of
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the 
Puppet-HPC Reference].

=== Generate keys

////
TODO: document the steps to generate the keys, certificates, and so on.
////
 
==== Password and keys for data encryption

As explained in the Puppet-HPC reference, sensitive data stored in the internal 
Git repository should be encrypted. Puppet-HPC uses the 2 following methods to
do so :

* Full sensitive files are encrypted with the `enc` plugin of the tool `Clara`
* Passwords are directly encrypted in Hiera input files using the Hiera-Eyaml 
functionnality

Consequently you will need 2 types of encryption keys :

* a PKCS7 key pair to set up Hiera-Eyaml,
* an AES key that is used by Clara to encrypt files, and by Puppet-HPC to decrypt
files. This password is referred to in Puppet-HPC as the `Cluster Decrypt Password`.

To generate the PKCS7 key pair, it is necessary to work on a node where Hiera-Eyaml has been installed manually. The full procedure to initially generate these keys
is explained in
http://edf-hpc.github.io/puppet-hpc/puppet_hpc_reference-0.1.html[the
Puppet-HPC Reference] in Sensitive Data Encryption, Bootstrap Procedure.

The PKCS7 key pair is expected to be tared, encoded and then stored in the
subdirectory `files/cluster1/eyaml` of the internal repository.

Furthermore, please refer to 
http://edf-hpc.github.io/clara/[Clara documentation] if you need more details on how it works.

==== Cluster keyring

The cluster must use a private cluster keyring. This keyring is used to
sign packages generated locally and in the local repositories.

You should generate it ans store it in the internal repository in the subdirectory
`files/cluster1/repo`. You will be asked for a
passphrase, this passphrase must be provided interactively when you call
`clara repo add|del`. The following command can be pretty long to
execute if you do not use a hardware Random Number Generator (RNG).

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 # LANG=C gpg --no-default-keyring --keyring files/repo/cluster_keyring.gpg --secret-keyring files/repo/cluster_keyring.secret.gpg --gen-key                       gpg (GnuPG) 1.4.18; Copyright (C) 2014 Free Software Foundation, Inc.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

gpg: keyring `files/repo/cluster_keyring.secret.gpg' created
gpg: keyring `files/repo/cluster_keyring.gpg' created
Please select what kind of key you want:
   (1) RSA and RSA (default)
   (2) DSA and Elgamal
   (3) DSA (sign only)
   (4) RSA (sign only)
Your selection? 1
RSA keys may be between 1024 and 4096 bits long.
What keysize do you want? (2048) 4096
Requested keysize is 4096 bits
Please specify how long the key should be valid.
         0 = key does not expire
      <n>  = key expires in n days
      <n>w = key expires in n weeks
      <n>m = key expires in n months
      <n>y = key expires in n years
Key is valid for? (0)
Key does not expire at all
Is this correct? (y/N) y

You need a user ID to identify your key; the software constructs the user ID
from the Real Name, Comment and Email Address in this form:
    "Heinrich Heine (Der Dichter) <heinrichh@duesseldorf.de>"

Real name: HPC Team Example cluster
Email address: hpc@example.com
Comment:
You selected this USER-ID:
    "HPC Team Example cluster <hpc@example.com>"

Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit? O
You need a Passphrase to protect your secret key.

passphrase not correctly repeated; try again.
We need to generate a lot of random bytes. It is a good idea to perform
some other action (type on the keyboard, move the mouse, utilize the
disks) during the prime generation; this gives the random number
generator a better chance to gain enough entropy.
..+++++
...........+++++
We need to generate a lot of random bytes. It is a good idea to perform
some other action (type on the keyboard, move the mouse, utilize the
disks) during the prime generation; this gives the random number
generator a better chance to gain enough entropy.
+++++
.+++++
gpg: key 241FB865 marked as ultimately trusted
public and secret key created and signed.

gpg: checking the trustdb
gpg: public key of ultimately trusted key 1F2607DD not found
gpg: public key of ultimately trusted key 94DEFA86 not found
gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model
gpg: depth: 0  valid:   3  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 3u
pub   4096R/241FB865 2016-05-19
      Key fingerprint = D192 11C0 2EB6 BE80 A3BC  7928 1CB4 3266 241F B865
uid                  HPC Team Example cluster <hpc@example.com>
sub   4096R/C7027D3A 2016-05-19
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Clara uses this key in its encrypted form. If the `clara enc` plugin is set,
 it is possible to use `clara enc encode` directly. Otherwise the
following command can perform the encryption:

----------------------------------------------------------------------------------------------------------------------
$ openssl aes-256-cbc -in cluster_keyring.secret.gpg -out cluster_keyring.secret.gpg.enc -k <cluster decrypt password>
----------------------------------------------------------------------------------------------------------------------


== Install admin node

////
TODO: document the steps to boot the admin node with rikenter and
install/configure minimal installation system in live.
////

== Install generic servers

////
TODO: document how-to install and configure generic  nodes.
////

== Configure Ceph

=== Architecture

image::src/img/ceph_architecture.svg[image]

A ceph cluster is created on the service nodes. A ceph cluster is made of four
kinds of daemons. All service nodes will have:

- *OSD*, Object Storage Daemons actually holding the content of the ceph
  cluster
- *RGW*, Rados GateWay (sometimes shortened radosgw) exposing an HTTP API like
  S3 to store and retrieve data in Ceph

Two other kind of service are only available on three of the service nodes:

- *MON*, Monitoring nodes, this is the orchestrators of the ceph cluster. A
  quorum of two active mon nodes must be maintained for the cluster to be
  available
- *MDS*, MetaData Server, only used by CephFS (the POSIX implementation above
  ceph). At least one must always be active.

With this configuration, any server can be unavailable. As long as at least two
servers holding critical services are availble, the cluster might survive
losing another non-critical server. 

=== Deployment

Deployment is based on a tool called `ceph-deploy`. This tool performs the
steps on a node to setup a ceph component. `ceph-deploy` is used to setup a
node, once the cluster is running, the configuration is reported in the Puppet
configuration in case it is re-deployed.

The reference configuration uses one disk (or hardware RAID LUN) to hold the
system (`/dev/sda`) and another to hold the Ceph OSD data and journal
(`/dev/sdb`).

Three or five nodes must be chosed to setup the *MON* and *MDS* services, the
remaining nodes are used only as *OSD* and *RadosGW* nodes.

To work, `ceph-deploy` must run on a node with a passwordless SSH connection to
the service nodes, directly to root or to a user with passwordless sudo.

The `ceph-deploy` tool will generate authentication keys for ceph. Once the
cluster is running, theses keys should be manually collected and encrypted with
`eyaml` to be included in the *hiera* configuration.

In the following example MONs/MDS are installed on nodes `clservice[2-4]` and a
node `clservice1` only has OSD and RGW.

==== Ceph-deploy init

----
# mkdir ceph-deploy
# cd ceph-deploy
# ceph-deploy new clservice2
----

==== Manually install packages

----
# ceph-deploy install --repo-url http://repo.hpc.example.com/scibian-hpc/scibian8/scibian8 --no-adjust-repos clservice1 clservice2 clservice3 clservice4
----

==== Install admin credentials

----
# ceph-deploy admin clservice1 clservice2 clservice3 clservice4
----

==== MON

----
for i in {2..4} ; do ceph-deploy mon add clservice${i} ; done
----

==== OSD

----
# for i in {1..4} ; do ceph-deploy disk zap clservice${i}:sdb ; done
# for i in {1..4} ; do ceph-deploy osd prepare clservice${i}:sdb ; done
----

==== MDS

----
# ceph-deploy mds create clservice2 clservice3 clservice4
----

==== RadosGW

----
# for i in {1..4} ; do ceph-deploy rgw create clservice${i} ; done
----

==== Libvirt RBD pool

The virtual machines will use a specific libvirt storage pool to store the disk
images. This libvirt storage pool uses ceph RBD, so a specific ceph pool is
necessary. This is not handled by `ceph-deploy`: 

----
# ceph osd pool create libvirt-pool 64 64
----

If the cluster has five OSDs or more, the numbers of PGs can be set to 128.

The client credentials must be manually generated:

----
# ceph auth get-or-create client.libvirt mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=libvirt-pool'
----

==== CephFS initialization

----
# ceph osd pool create cephfs_data 64
pool 'cephfs_data' created
# ceph osd pool create cephfs_metadata 64
pool 'cephfs_metadata' created
# ceph fs new cephfs cephfs_metadata cephfs_data
new fs with metadata pool 15 and data pool 14
----

If the cluster has five OSDs or more, the numbers of PGs can be set to 128 for
data and metadata pool.

==== RadosGW S3 

A user must be created to access the RadosGW S3 API:

----
# radosgw-admin user create --uid=hpc-config --display-name="HPC Config push"
----

This commands gives an `access_key` and a `secret_key` that can be used by
`hpc-config-push(1)` or `s3cmd(1)`.

To work properly with Amazon S3 tools and consul DNS, RadosGW must be
configured to accept requests on `rgw.service.virtual` and on
`<bucket_name>.service.virtual`. To configure this, it is necessary to
re-define the default realm, region and zonegroup.

The region is configured by writing a JSON region file (`rgw-region.json`):

[source]
----
include::examples/rgw-region.json[]
----

----
# radosgw-admin realm create --rgw-realm=default --default
# radosgw-admin region set --infile rgw-region.json
# radosgw-admin region --rgw-zonegroup=default default
# radosgw-admin zonegroup default --rgw-zonegroup=default
# radosgw-admin period get
# radosgw-admin period update --commit
----

After this step the RadosGW daemons must be restarted on every nodes.

----
clush -w clservice[1-4] 'systemctl restart ceph-radosgw@rgw.${HOSTNAME}.service''
----

==== Hiera

When the cluster is initalized, the authentication keys must be reported in the
Hiera configuration.

[source]
----
include::examples/hiera-ceph.yaml[]
----

=== Ceph server re-installation

In case of generic service node reinstallation after the initial configuration,
bootstrap steps may be necessary:

- *MDS* and *RadosGW*, those services have no state outside of Rados, so no
  additional bootstrap is necessary
- *Mon* Always necessary to bootstrap
- *OSD* Must be bootstraped if the OSD volume (`/dev/sdb`) is lost.

== Re-install admin node

////
TODO: document how-to install admin node with generic service nodes.
////

== Install virtual machines

////
TODO: document how-to install and configure virtual machines with references to
[bootstraping guides] for service.
////

== Build diskless image

Diskless nodes use squashfs images downloaded at boot. A simple way to
generate these images is to use https://github.com/edf-hpc/clara[`clara`].
Once configured, `clara image create` can be used to build Debian image(s).

Clara can install a selected list of packages into the image, manage
the repositories used and add specific files and directories.

Example:
-------------------------------------------------------------------------------
# clara image create scibian8
-------------------------------------------------------------------------------

Clara can also generate an `initrd` (initial ramdisk) to use with the image.
It can download the image either by http or bitorrent protocol.

Example:
-------------------------------------------------------------------------------
# clara image initrd scibian8
-------------------------------------------------------------------------------

Initrd must be regenerated each time the kernel image is modified.

See the http://edf-hpc.github.io/clara/[clara documentation]  for a full
list of options and possibilities.

Every time the image and the initrd are regenerated, they must be made
available to the deployment system.

== Boot nodes

To remotely start the nodes, you can use https://github.com/edf-hpc/clara[`clara`].
The ipmi plugin from clara is used for powering on/off selected nodes,
managing the boot node mode (if you want to install OS on disk or load it in RAM)
and accessing to the remote console.

See the http://edf-hpc.github.io/clara/[clara documentation] for a full
list of options and possibilities.

The commands from the following example, show how to reboot a node with
the boot menu, choose the operating system and boot mode between "diskless"
or "on disk".

Example:
-------------------------------------------------------------------------------
# clara ipmi pxe <node>
# clara ipmi reboot <node>
# clara ipmi connect <node>
-------------------------------------------------------------------------------

After the last command, when the login prompt is shown, you can connect on node
as root and monitor the progress of the puppet configuration by running
the following command:

-------------------------------------------------------------------------------
# journalctl -f -u hpc-config-apply
-------------------------------------------------------------------------------

When the network is up in the node you can also disconnect from the console
(press on "!" and "." keys as explained on the clara documentation) and
connect to the node via ssh.
