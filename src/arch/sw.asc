== Software architecture

=== Overview

image::src/img/arch_sw_overview.svg[image]

==== Functions

The software configuration of the cluster aims to deliver a set of functions.
Functions can rely on each other, for example, the disk installer uses the
configuration management to finish the post-install process.

Main functions provided by a Scibian HPC cluster are:

- *Configuration Management*, distributing and applying the configuration to
  the nodes
- *Disk Installer*, install an OS from scratch on the nodes disks through the
  network
- *Diskless Boot*, boot a node with a live diskless OS through the network
- *Administrator Tools*, tools and services used by the system administrator to
  operate the cluster
- *User Tools*, tools and services used by the end users 

The Scibian HPC Cluster will use a set of Services to deliver a particular
function. If a cluster can provide *Configuration Management* and a *Disk
Installer*, it is able do operate even if it can't do something useful for the
users. These two core functions permits to create a self sufficient cluster
that will be used to provide other functions.

==== Services

The software services of the cluster are sorted into two broad categories:

- *Base Services*, necessary to provide core functions: install and configure a
  physical or virtual machine
- *Additional Services* to boot a diskless (live) machine, provide all end user
  services (batch, user directory, licenses...), and system services not
  mandatory to install a machine (monitoring, metrics...)

The Base Services run on a set of physical machines that are almost identical,
those hosts are called *Service Nodes*. The services are setup to work reliably
if any of the service node is down. This means that a service node can be
re-installed by other active service nodes.

Additional Services can be installed on a set of other hosts physical or
virtual. VMs (Virtual Machines) are usually used because those services don't
need a lot of raw power and the agility provided by virtual machines (like live
host migration) are often an advantage.

If the cluster is using virtualized machines for Additional Services, the
service nodes must also provide a consistent virtualization platform (storage
and hosts). In the reference architecture, this is provided with Ceph RBD and
Libvirtd running on service nodes.

A particular service runs on service nodes even if it is not mandatory for Disk
Installer or Config Management: the low latency network manager (Subnet Manager
for Infiniband, Fabric Manager for Intel Omni-Path). This exception is due to the
fact that this particular service needs raw access to the low latency network.

In the Puppet configuration, services are usually associated with *profiles*.
For example, the puppet configuration configures the *DNS Server* service with
the profile: ``profiles::dns::server``.

=== Base Services

==== Infrastructure

Infrastructure Services provides basic network operations:

- DHCP and TFTP for PXE Boot
- DNS servers, with forwarding for external zones
- NTP servers, synchronized on external servers

These services are configured the same way and running on each service nodes.

[[arch-soft-consul]]
==== Consul

Consul is a service that permits to discover available services in the cluster.
Client will query a special DNS entry (`xxx.service.virtual`) and the DNS
server integrated with Consul will return the IP address of an available instance.

==== Ceph

Ceph provides an highly available storage system for all system needs. Ceph has
the advantage to work with internal storage on service nodes and to not need a
storage system shared between servers (NAS or SAN).

Ceph provides:

- Rados Block Devices (RBD) that is used to store Virtual Machines disk images
- Rados GateWay to provide storage for configuration management, Amazon S3
  compatible REST API for write operations and plain HTTP for read.
- Ceph FS provides a POSIX filesystem that is used for Slurm Controller state
  save location

All service nodes runs actual storage daemons (OSD) and Rados Gateway
instances. Only three of the service nodes run the Ceph Monitoring (Mon)
servers and Ceph FS MetaData Servers (MDS).

==== Libvirt/KVM

Service nodes are also the physical hosts for the Virtual Machines of the
cluster. Libvirt is used in combination with QEMU/KVM to configure the VMs. A
Ceph RBD pool is used to store the image of the VMS. With this configuration,
the only state on a service node is the VM definition.

Integration with Clara makes it easy to move VMs between nodes. 

==== HTTP Secret and Boot

The node boot process needs a configuration obtained through HTTP and computed
by a CGI (in Python). This is hosted on the service nodes and served by Apache.
This is also used to serve files like the kernel, initrd and pre-seeded
configuration.

A special Virtual Host on the Apache configuration is used to serve secrets
(Hiera Eyaml keys). This VHost is configured to only serve the files on a
specific port. This port is only accessible if the client connects from a port
below 1024 (is root), this is enforced by a Shorewall rule.

==== APT proxy

There is no full repository mirror on the cluster. APT is configured to use a
proxy that will fetch data from external repositories and cache it. This permit
to have always up to date packages without overloading external repositories
and without having to maintain mirror sync (internally and externally).

==== Logs

Logs from all nodes are forwarded to a Virtual IP address running on service
nodes. The local rsyslog daemon will centralize those log and optionally
forward it to an external location.

==== Low Latency Network Manager

The Low Latency Network Manager (Infiniband Subnet Manager or Intel Omni-Path
Fabric Manager) is not mandatory to achieve the feature set of Base Services
(Configuration Management and Disk Installation) but it must run on a physical
machine, so it is grouped with the Base Services to run on the service nodes.

=== Additional Services

==== LDAP

There is no standalone LDAP servers configured. The servers are replica from an
external directory. This means that both are configured independently and are
accessed only for read operations.

If the organization uses Kerberos, all Kerberos requests and passwords checks
are done directly by the external Kerberos server.

==== Bittorrent

Diskless image files are downloaded by the nodes with the BitTorrent protocol.
The cluster provide a redundant tracker service with OpenTracker and two server
machines are configured to allways seed the images.

An Apache server is used to serve the torrent files for the Diskless images
(HTTP Live).

==== Slurm

Slurm provides the job management service for the cluster. The controller
service (SlurmCtld) runs in an Active/Passive configuration on a pair of
servers (*batch* nodes). The state is shared between the controller nodes
thanks to a CephFS POSIX directory.

The SlurmDBD service also runs on these two servers.

==== MariaDB/Galera

SlurmDBD uses a MySQL like database to store accounting information and limits.
On Scbian HPC Clusters this is provided by a MariaDB/Galera cluster which
provide an Active/Active SQL server compatible with MySQL.

This cluster is usually co-located with SlurmDBD service and Slurm Controllers
(*batch* nodes). 

==== Relays

Additional services include a set of relay service to the outside of the
cluster for:

- Email (Postfix Relay)
- Network (NAT configured by Shorewall)
- Metrics (Carbon C Relay)
- ...

==== Monitoring

Cluster monitoring is done by Icinga2, the cluster is integrated inside an
organization Icinga infrastructure. The cluster hosts a redundant pair of
monitoring satellites that checks the nodes. The monitoring master is external
to the cluster.

=== High-Availability

All services running on the cluster should be highly available (HA). Some services
not critical for normal cluster operation can be not highly available, but this
should be avoided if possible.

The following sections lists the different techniques used to achieve
high-availability on the cluster services.

==== Stateless

Stateless services are configured the same way on all servers and will give the
same answer to all requests. These services include:

- *DHCP*
- *TFTP*
- *NTP*
- *DNS*
- *LDAP Replica*
- *HTTP Secret*
- *HTTP Boot*
- *HTTP Live*
- *Ceph RadosGW*
- *APT Proxy*
- *Carbon Relay*
- *Bittorrent Tracker*
- *Bittorrent Seeder*
- *SMTP Relay*

Clients can be provided a list of potential servers that will be tried in turn.
If the client do not automatically accept multiple servers, it is possible to
use the Consul service to get a DNS entry (``xxx.service.virtual``) that will
always point to an available instance of the service.

As a last resort and for services that don't need Active/Active (Load
Balancing) capabilities, it is possible to user a Virtual IP address (VIP).
*HTTP Live* and *Carbon Relay* uses this technique.

==== Native Active/Active

Some services have native internal mechanisms to share state between the
servers. Contacting any server will have the same effect on the state of the
service, or the service has an internal mechanism to get the right server.
These services behave this way:

- *Ceph Rados*
- *MariaDB/Galera*
- *Consul*

==== Native Active/Passive

Services that have only one active server at any time, but the mechanism to
select the active server is internal to the service. This means all servers are
launched in the same way and not by an external agent like Keepalived or
Pacemaker/Corosync. Services using this technique are:

- *Ceph MDS* (Posix CephFS server)
- *Slurm Controller*
- *Omni-Path Fabric Manager* or *Infiniband Subnet Manager*


