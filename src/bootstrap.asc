= Bootstrap procedures

This chapter contains all the procedures to boostrap all the crucial
services for a Scibian HPC system: LDAP, Ceph, MariaDB with Galera,
SlurmDBD, etc.

== LDAP bootstrap

As stated in <<arch-extsrv,external services>> section of the Reference
Architecture chapter, a central LDAP directory server external to the Scibian
HPC cluster is required. The LDAP directory server on the cluster is just is a
_replica_ of this central external server.

The Puppet-HPC `openldap` module expects a LDIF file containing a full dump of
the LDAP replica configuration. The easiest way to produce this bootstrap LDIF
file is to install and configure an LDAP server replica manually and dump the
live configuration.

First, install an LDAP server with common LDAP utilities:

----
# apt-get install slapd ldap-utils
----

Select the HDB database backend. Then, configure the base DN, the domain name,
the organization name according to your environment, and set the administration
password.

Write the LDAP replication configuration LDIF file `syncrepl_config.ldif`,
similarly to this example:

----
dn: olcDatabase={1}hdb,cn=config
changetype: modify
add: olcSyncrepl
olcSyncrepl: rid=001 provider=<LDAP_SERVER_URL> bindmethod=simple timeout=0
  tls_cacert=<CA_CRT_CHAIN>
  network-timeout=0 binddn="<BIND_DN>" credentials="<BIND_PASSWORD>"
  searchbase="dc=calibre,dc=edf,dc=fr"
  schemachecking=on type=refreshAndPersist retry="60 +"
-
add: olcUpdateref
olcUpdateref: <LDAP_SERVER_URL>
----

Where:

* `LDAP_SERVER_URL` is the URL to the organization central LDAP server, _ex:_
  `ldaps://ldap.company.tld`.
* If using TLS/SSL, `CA_CRT_CHAIN` is the absolute path to the CA certificate
  chain (up-to root CA certificate), _ex:_
  `/usr/local/share/ca-certificates/ca-chain.crt`
* `BIND_DN` is the replication user DN, _ex:_ `cn=replication,dc=company,dc=tld`
* `BIND_PASSWORD` is the password of the replication user

Inject this LDIF replication configuration file into the LDAP server:

----
# ldapmodify -a -Y EXTERNAL -H ldapi:// -f syncrepl_config.ldif
----

Using the same technique, configure to your needs the indexes, ACLs, TLS/SSL,
password policy, kerberos, etc. Finally, generate the full LDAP config dump
with:

----
# slapcat -b cn=config > config_replica.ldif
----

or:

----
# ldapsearch -Y EXTERNAL -H ldapi:/// -b cn=config > config-replica.ldif
----

The `config_replica.ldif` file must be deployed encrypted within Puppet-HPC
private files directory. Please refer to Puppet-HPC Reference Documentation
for more details.

After a fresh installation the cluster's services virtual machines that host the
LDAP directory replicas, the `config_replica.ldif` is deployed by Puppet and the
LDAP replication must be bootstraped with this script:

----
# make_ldap_replica.sh
----

The script will ask you to confirm by typing `YES` and press enter.

== MariaDB/Galera bootstrap

The Puppet-HPC `mariadb` module configures an active/active MariaDB cluster
based on galera replication library. On the service virtual machines that host
this database system, the corresponding `mysql` system service (MariaDB is
initially a fork of MySQL and the old-name still appears in some places) will
not start unless it is already started on another service virtual machine. If it
is not running anywhere else, the service must bootstraped like this:

----
# echo MYSQLD_ARGS=--wsrep-new-cluster > /etc/default/mysql
# systemctl start mysql.service
# rm /etc/default/mysql
----

Once the service is started on all service virtual machines, you can check the
cluster replication status with:

----
# mysql --defaults-extra-file=/etc/mysql/debian.cnf \
  -e "SELECT VARIABLE_VALUE as cluster_size \
      FROM INFORMATION_SCHEMA.GLOBAL_STATUS \
      WHERE VARIABLE_NAME='wsrep_cluster_size'"
----

This result must be the number of expected active nodes in the MariaDB/Galera
cluster (_ex:_ 2).

== SlurmDBD bootstrap

After its first installation on the cluster, the SlurmDBD accounting database is
empty. First, the cluster must be created in the database:

----
# sacctmgr add cluster <name>
----

Where `<name>` is the name of the cluster.

Then, once the
https://github.com/edf-hpc/slurm-llnl-misc-plugins/tree/master/sync-accounts[`sync-accounts`]
utility is configured, run it to create all accounts and users:

----
# slurm-sync-accounts
----

Then, it is possible to create https://slurm.schedmd.com/qos.html[QOS] and
configure
https://slurm.schedmd.com/priority_multifactor.html#fairshare[fair-share]
depending upon your needs.

== Ceph

[[bootstrap-ceph-mon]]
=== Mon

After the reinstallation of one of the generic service nodes with a mon, it
must be re-initialized. This procedure only works on a running cluster, the
initial mon creation uses another command.

From an *admin* node:

--------
# cd <ceph deploy directory>
# ceph-deploy --overwrite-conf mon add <mon hostname>
--------

=== OSD

This procedure only applies if the content of an OSD volume is lost. If the
node is reinstalled without erasing the content of the OSD volume, the
configuration in puppet will be enough to start the osd volume again.

The relevant OSD ID can be retrieved with:

----
# ceph osd tree
----

Before doing this procedure, make sure the OSD is really down and not mounted
on the OSD node.

==== Removing old OSD

The old OSD must be removed from the configuration (stored by the MON).

----
# ceph osd crush remove osd.X
# ceph auth del osd.X
# ceph osd rm X
----

==== Re-creating the OSD

----
# cd <ceph deploy directory>
# ceph-deploy osd prepare clserviceY:sdb
# ceph-deploy disk zap clserviceY:sdb
----

The OSD id and authentication key should be updated on the hiera configuration.
In most cases, the new OSD will take the same ID as the old one. You can get
the new ID and the new key with:

----
# ceph osd tree
# ceph auth print-key osd.X
----
