=== Cluster definition

The cluster specific layers of the Hiera repository must be initialized with a
sufficient description of the HPC cluster. This description is the _cluster
definition_.

==== Networks definition

A specific layer in the hiera repository stack is dedicated to all the networks
settings of the HPC cluster. This layer is defined in file
`$ADMIN/hpc-privatedata/hieradata/$CLUSTER/network.yaml`. Initialize this file
with the following content:

[source,yaml]
----
include::../../examples/initial_network.yaml[]
----

The first `network::{ip,opa}_enable` define which high-performance interconnect
network technology is involved in the HPC cluster (InfiniBand or Intel
Omni-Path).

The `net::*` parameters and `net_topology` hash basically define the adressing
maps of the various IP networks of the clusters, along with some metadata such
as the network hostname prefixes, the DHCP dynamic pools and the firewall zones
associated to these IP networks.

The `network::bonding_options` and `network::bridge_options` hashes respectively
define all the network interfaces bondings and virtual bridges involved on the
nodes of the HPC cluster. Note that these settings are global to all nodes.

The `master_network` hash defines the list of nodes and all their network
interfaces with the associated IP addresses, network hostnames and eventually
MAC addresses (on the administration and bmc networks).

Finally, the `vips` hash define the virtual highly-available IP addresses (VIP)
managed by nodes of the HPC cluster.

Initially, the YAML file must contain all the IP network definitions and the
network settings of all the generic service nodes with their VIP.

==== General cluster settings

The cluster specific general parameters and services settings are located in
file `$ADMIN/hpc-privatedata/hieradata/$CLUSTER/cluster.yaml`. Initialize this
file with the following content:

[source,yaml]
----
include::../../examples/initial_cluster.yaml[]
----

////
NOTE: most of the ugly hash with raw d-i preseed could be removed once this bug
is fixed:
  https://github.com/edf-hpc/puppet-hpc/issues/81
////

Additionally to some general parameters (`cluster_prefix` and `user_groups`),
the initial version of this file notably contains the configuration of the base
services required to install nodes on disk (DNS, TFTP, HTTP, DHCP, Debian
installer, etc).

Also, in order to prevent user to access the cluster during the installation
process, it is recommended to enable the maintenance mode in this file:

[source,yaml]
----
profiles::access::maintenance_mode: true
----
